{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf005d7",
   "metadata": {},
   "source": [
    "## Example 2: Executing a Search and Downloading the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a2d2d",
   "metadata": {},
   "source": [
    "In `example_1.ipynb`, we saw how to download an e-print using its arXiv ID. This is a useful feature, but is somewhat limited by the fact that we need to know the ID ahead of time. Instead, what if we want to search for any e-prints that contain certain keywords or authors (for example), and download the results of the search? This functionality is covered by the other main class of `pyxiv`: the `Search` class. In this example, we'll get comfortable with using `Search` to find and download e-prints. To start, we import `pyxiv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf538d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import pyxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f805c5",
   "metadata": {},
   "source": [
    "To view the basics of the `Search` class, including its parameters and intended usage, we can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8d358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mpyxiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquery\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstart_date\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mend_date\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'today'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_results\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msort_order\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'descending'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Class for searching through e-prints submitted to arxiv.org using\n",
       "the [arXiv API](https://info.arxiv.org/help/api/index.html). Once\n",
       "e-prints are found that match the search criteria, they can be\n",
       "downloaded locally as .pdf files.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "query : str\n",
       "    The core part of a query compatible with the arXiv API.\n",
       "    Instructions on how to construct queries are available\n",
       "    [here](https://info.arxiv.org/help/api/user-manual.html#query_details).\n",
       "    `query` should be formatted as `'au:del_maestro+AND+ti:checkerboard'`,\n",
       "    for example. Note that the prefix '`query?search_query=`'\n",
       "    is *not* included; this is handled automatically by the\n",
       "    `Search` class. As well, subsequent query specifications like\n",
       "    `'&start=0&max_results=1'` are handled by other arguments of\n",
       "    `Search`. See `keyword_query` and `author_query` for pre-set\n",
       "    query types.\n",
       "start_date : str\n",
       "    The earliest possible submission date for an e-print included\n",
       "    in the search results (i.e. e-prints submitted prior to\n",
       "    `start_date` are excluded). One of `'today'`, `'yesterday'`,\n",
       "    or an ISO-formatted (`'YYYY-MM-DD'`) string. Note that\n",
       "    submission date is not a search field available to the arXiv\n",
       "    API; filtering by start date is therefore post-processing that\n",
       "    occurs after the API has been queried.\n",
       "end_date : str, default 'today'\n",
       "    The latest possible submission date for an e-print included\n",
       "    in the search results (i.e. e-prints submitted after\n",
       "    `start_date` are excluded). One of `'today'`, `'yesterday'`,\n",
       "    or an ISO-formatted (`'YYYY-MM-DD'`) string. Note that\n",
       "    submission date is not a search field available to the arXiv\n",
       "    API; filtering by end date is therefore post-processing that\n",
       "    occurs after the API has been queried.\n",
       "max_results : int, default 250\n",
       "    The number of e-prints returned by the query of the arXiv API.\n",
       "    Note that since filtering by date occurs post-query, the value\n",
       "    of `max_results` will typically not correspond to the number\n",
       "    of e-prints listed in the search results given by this class.\n",
       "sort_order : str, default 'descending'\n",
       "    The sort order used by the query of the arXiv API. If set to\n",
       "    'descending', it sorts from newest e-prints to oldest; if set\n",
       "    to 'ascending' it sorts from oldest e-prints to newest.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\rusla\\dropbox\\23-github\\projects\\chat-research\\pyxiv\\search.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyxiv.Search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04b212",
   "metadata": {},
   "source": [
    "As we can see in the docstring for `Search`, it has two mandatory arguments: `query` and `start_date`. `query` is a string that specifies what to search for, formatted according to the requirements of the [arXiv API](https://info.arxiv.org/help/api/index.html). `start_date` is an ISO-formatted date string (i.e. a string of the form `\"YYYY-MM-DD\"`) that sets the date at which the search results begin. For convenience, `start_date` can also take the values `\"today\"` and `\"yesterday\"`.\n",
    "\n",
    "There are also three optional arguments: `end_date`, `max_results`, and `sort_order`. `end_date` is the date at which the search results end, and is formatted in an identical manner to `start_date`; it is set to `\"today\"` by default. `max_results` is an integer that sets the number of e-prints returned by the arXiv API; it is set to `250` by default.\n",
    "\n",
    "Here, it is necessary to distinguish between the search results returned by `Search` and the results returned by the arXiv API. Unfortunately, queries of the arXiv API cannot sort by specific dates. Therefore, filtering to match the requirements of `start_date` and `end_date` is a post-query process. This is important because, in general, the number of e-prints returned by `Search` will be less than `max_results`, because any e-prints found by the arXiv API that were submitted before `start_date` or after `end_date` are ignored by `Search`. In fact, if the number of e-prints returned by `Search` is equal to `max_results`, then it is likely necessary to increase `max_results`, because the query of the arXiv API is ending before the date limits are reached. It is recommended to increase `max_results` from its default value of 250 if we are searching for keywords with a high frequency of publication  or across a long time span.\n",
    "\n",
    "Finally, `sort_order` controls the order in which the arXiv API search through e-prints; it is `'descending'` by default. Setting `sort_order` to ascending is only recommended if we want very old e-prints, since the query will start with oldest submissions first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7fe88",
   "metadata": {},
   "source": [
    "To illustrate how `Search` works, suppose we want to find and download all papers submitted to the astro-ph.EP category between March 14, 2023 and May 4, 2023 that are about exoplanets and machine learning. We can accomplish this goal by creating the following instance of `Search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a285c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring query results from the arXiv API...\n",
      "Results acquired in 2.3 sec.\n"
     ]
    }
   ],
   "source": [
    "search = pyxiv.Search(\n",
    "    query      = \"all:exoplanets AND all:machine learning AND cat:astro-ph.EP\",\n",
    "    start_date = \"2023-03-14\",\n",
    "    end_date   = \"2023-05-04\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaa31c",
   "metadata": {},
   "source": [
    "We see that we have converted our desired search criteria (e-prints involving exoplanets and machine learning submitted to astro-ph.EP) to a query string. This conversion was done like so:\n",
    "- We want to search for e-prints involving exoplanets. Since we are not restricting ourselves to a particular data field (like author, title, etc.), we use the `\"all\"` field, and get `\"all:exoplanets\"`.\n",
    "- We want to search for e-prints involving machine learning. Since we are not restricting ourselves to a particular data field (like author, title, etc.), we use the `\"all\"` field, and get `\"all:exoplanets\"`.\n",
    "- We want to search for e-prints involving exoplanets and machine learning. Therefore, we combine the previous strings using `\"AND\"`, giving us `\"all:exoplanets AND all:machine learning\"`.\n",
    "- We want to search specifically in the astro-ph.EP category. Thus, we append `\"cat:astro-ph.EP\"` to the previous string using `\"AND\"`, yielding `\"all:exoplanets AND all:machine learning AND cat:astro-ph.EP\"`.\n",
    "\n",
    "A full description of how queries are formatted can be found at the arXiv API [query instructions manual](https://info.arxiv.org/help/api/user-manual.html#51-details-of-query-construction). Note that in the examples given in the instructions manual, the queries are full URLs prefixed with `http://export.arxiv.org/api/query?search_query=` and occasionally suffixed with other specifiers like `&start=0&max_results=1`. These should NOT be included in the string passed to the `query` argument of `Search`, as they are handled automatically behind the scenes.\n",
    "\n",
    "As well, we may notice that the query used here contains spaces, while example queries in the instructions manual use + to join words. The query with spaces works, though, because the `Search` class automatically replaces all spaces with pluses behind the scenes. Therefore, the queries `\"all:exoplanets AND all:machine learning AND cat:astro-ph.EP\"` and `\"all:exoplanets+AND+all:machine+learning+AND+cat:astro-ph.EP\"` are seen as identical by `Search` (but not by the arXiv API alone!), with the former being preferred for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82af8db",
   "metadata": {},
   "source": [
    "Now that `search` has been created, we can view the search results by calling the `results` method. To learn more about the `results` method, we can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1234b307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mpyxiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'low'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Method that generates a string containing the results of\n",
       "the search.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "detail : str, default 'low'\n",
       "    Controls the level of detail included in the summary of\n",
       "    each `ePrint` object. If set to `'high'`, all relevant\n",
       "    fields are included. If set to `'low'`, `abstract`,\n",
       "    `comment`, `all_categories`, `doi`, `journal_ref`, and\n",
       "    `date_updated` are omitted.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Union[str, None]\n",
       "    If the search yields results, a string that\n",
       "    concatenates `ePrint.summary(detail=detail)` for each\n",
       "    e-print is returned. If the search yields no results,\n",
       "    `None` is returned and a message to help with\n",
       "    troubleshooting is printed.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\rusla\\dropbox\\23-github\\projects\\chat-research\\pyxiv\\search.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyxiv.Search.results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81189c76",
   "metadata": {},
   "source": [
    "Next, we call the `results` method on `search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59770ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specified query yielded 5 results:\n",
      "================================================\n",
      "arXiv.org e-Print 2303.09335v2\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "ExoplANNET: A deep learning algorithm to detect and identify planetary\n",
      "  signals in radial velocity data\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "L. A. Nieto, R. F. Díaz\n",
      "\n",
      "Primary category: astro-ph.EP\n",
      "URL: https://arxiv.org/pdf/2303.09335v2.pdf\n",
      "Submitted: 2023-03-16\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2303.12925v1\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "A Catalogue of Exoplanet Atmospheric Retrieval Codes\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "Ryan J. MacDonald, Natasha E. Batalha\n",
      "\n",
      "Primary category: astro-ph.EP\n",
      "URL: https://arxiv.org/pdf/2303.12925v1.pdf\n",
      "Submitted: 2023-03-22\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2304.00224v1\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "The CARMENES search for exoplanets around M dwarfs -- A deep transfer\n",
      "  learning method to determine Teff and [M/H] of target stars\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "A. Bello-García, V. M. Passegger, J. Ordieres-Meré, A. Schweitzer, J. A. Caballero, A. González-Marcos, I. Ribas, A. Reiners, A. Quirrenbach, P. J. Amado, V. J. S. Béjar, C. Cifuentes, Th. Henning, A. Kaminski, R. Luque, D. Montes, J. C. Morales, S. Pedraz, H. M. Tabernero, M. Zechmeister\n",
      "\n",
      "Primary category: astro-ph.SR\n",
      "URL: https://arxiv.org/pdf/2304.00224v1.pdf\n",
      "Submitted: 2023-04-01\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2304.14283v1\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "Distinguishing a planetary transit from false positives: a\n",
      "  Transformer-based classification for planetary transit signals\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "Helem Salinas, Karim Pichara, Rafael Brahm, Francisco Pérez-Galarce, Domingo Mery\n",
      "\n",
      "Primary category: astro-ph.EP\n",
      "URL: https://arxiv.org/pdf/2304.14283v1.pdf\n",
      "Submitted: 2023-04-27\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2305.02470v2\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New\n",
      "  Exoplanets Using The Multiplicity Boost of ExoMiner\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "Hamed Valizadegan, Miguel J. S. Martinho, Jon M. Jenkins, Douglas A. Caldwell, Joseph D. Twicken, Stephen T. Bryson\n",
      "\n",
      "Primary category: astro-ph.EP\n",
      "URL: https://arxiv.org/pdf/2305.02470v2.pdf\n",
      "Submitted: 2023-05-04\n"
     ]
    }
   ],
   "source": [
    "results = search.results()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7e888",
   "metadata": {},
   "source": [
    "Note that the string returned by the `results` method appears to be a sequence of strings returned by the `ePrint` class's `summary` method, as seen in `example_1.ipynb`. This is indeed the case, since `Search` internally generates a list of `ePrint` objects.\n",
    "\n",
    "In a manner analogous to the `summary` method, the `results` method has an optional `detail` argument, set to `\"low\"` by default. Setting `detail = \"high\"` adds information about abstracts, comments, any additional categories the e-prints are included in, DOIs (if applicable), journal references (if applicable), and the dates the e-prints were last updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c58cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specified query yielded 5 results:\n",
      "================================================\n",
      "arXiv.org e-Print 2303.09335v2\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "ExoplANNET: A deep learning algorithm to detect and identify planetary\n",
      "  signals in radial velocity data\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "L. A. Nieto, R. F. Díaz\n",
      "\n",
      "Abstract\n",
      "--------\n",
      "The detection of exoplanets with the radial velocity method consists in\n",
      "detecting variations of the stellar velocity caused by an unseen sub-stellar\n",
      "companion. Instrumental errors, irregular time sampling, and different noise\n",
      "sources originating in the intrinsic variability of the star can hinder the\n",
      "interpretation of the data, and even lead to spurious detections. In recent\n",
      "times, work began to emerge in the field of extrasolar planets that use Machine\n",
      "Learning algorithms, some with results that exceed those obtained with the\n",
      "traditional techniques in the field. We seek to explore the scope of the neural\n",
      "networks in the radial velocity method, in particular for exoplanet detection\n",
      "in the presence of correlated noise of stellar origin. In this work, a neural\n",
      "network is proposed to replace the computation of the significance of the\n",
      "signal detected with the radial velocity method and to classify it as of\n",
      "planetary origin or not. The algorithm is trained using synthetic data of\n",
      "systems with and without planetary companions. We injected realistic correlated\n",
      "noise in the simulations, based on previous studies of the behaviour of stellar\n",
      "activity. The performance of the network is compared to the traditional method\n",
      "based on null hypothesis significance testing. The network achieves 28 % fewer\n",
      "false positives. The improvement is observed mainly in the detection of\n",
      "small-amplitude signals associated with low-mass planets. In addition, its\n",
      "execution time is five orders of magnitude faster than the traditional method.\n",
      "The superior performance exhibited by the algorithm has only been tested on\n",
      "simulated radial velocity data so far. Although in principle it should be\n",
      "straightforward to adapt it for use in real time series, its performance has to\n",
      "be tested thoroughly. Future work should permit evaluating its potential for\n",
      "adoption as a valuable tool for exoplanet detection.\n",
      "\n",
      "Comment: Accepted for publication; Corrected typos; Added section 6.1 with a\n",
      "  robustness analysis of the method; Added section 6.2 with tests on a real\n",
      "  time series; Added section 6.3 with a more detailed analysis of the caution\n",
      "  of the network around activity periods; Added other tested models to the\n",
      "  appendix\n",
      "Primary category: astro-ph.EP\n",
      "All categories: astro-ph.EP, astro-ph.IM, cs.LG\n",
      "URL: https://arxiv.org/pdf/2303.09335v2.pdf\n",
      "DOI: https://doi.org/10.1051/0004-6361/202346417\n",
      "Journal reference: A&A 677, A48 (2023)\n",
      "Submitted: 2023-03-16\n",
      "Last updated: 2023-07-01\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2303.12925v1\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "A Catalogue of Exoplanet Atmospheric Retrieval Codes\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "Ryan J. MacDonald, Natasha E. Batalha\n",
      "\n",
      "Abstract\n",
      "--------\n",
      "Exoplanet atmospheric retrieval is a computational technique widely used to\n",
      "infer properties of planetary atmospheres from remote spectroscopic\n",
      "observations. Retrieval codes typically employ Bayesian sampling algorithms or\n",
      "machine learning approaches to explore the range of atmospheric properties\n",
      "(e.g., chemical composition, temperature structure, aerosols) compatible with\n",
      "an observed spectrum. However, despite the wide adoption of exoplanet retrieval\n",
      "techniques, there is currently no systematic summary of exoplanet retrieval\n",
      "codes in the literature. Here, we provide a catalogue of the atmospheric\n",
      "retrieval codes published to date, alongside links to their respective code\n",
      "repositories where available. Our catalogue will be continuously updated via a\n",
      "Zenodo archive.\n",
      "\n",
      "Comment: 5 pages, 1 giant Table. Published in RNAAS. Live catalogue will be\n",
      "  updated at https://doi.org/10.5281/zenodo.7675743\n",
      "Primary category: astro-ph.EP\n",
      "All categories: astro-ph.EP, astro-ph.IM\n",
      "URL: https://arxiv.org/pdf/2303.12925v1.pdf\n",
      "DOI: https://doi.org/10.3847/2515-5172/acc46a\n",
      "Journal reference: RNAAS (2023): 7, 54\n",
      "Submitted: 2023-03-22\n",
      "Last updated: 2023-03-22\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2304.00224v1\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "The CARMENES search for exoplanets around M dwarfs -- A deep transfer\n",
      "  learning method to determine Teff and [M/H] of target stars\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "A. Bello-García, V. M. Passegger, J. Ordieres-Meré, A. Schweitzer, J. A. Caballero, A. González-Marcos, I. Ribas, A. Reiners, A. Quirrenbach, P. J. Amado, V. J. S. Béjar, C. Cifuentes, Th. Henning, A. Kaminski, R. Luque, D. Montes, J. C. Morales, S. Pedraz, H. M. Tabernero, M. Zechmeister\n",
      "\n",
      "Abstract\n",
      "--------\n",
      "The large amounts of astrophysical data being provided by existing and future\n",
      "instrumentation require efficient and fast analysis tools. Transfer learning is\n",
      "a new technique promising higher accuracy in the derived data products, with\n",
      "information from one domain being transferred to improve the accuracy of a\n",
      "neural network model in another domain. In this work, we demonstrate the\n",
      "feasibility of applying the deep transfer learning (DTL) approach to\n",
      "high-resolution spectra in the framework of photospheric stellar parameter\n",
      "determination. To this end, we used 14 stars of the CARMENES survey sample with\n",
      "interferometric angular diameters to calculate the effective temperature, as\n",
      "well as six M dwarfs that are common proper motion companions to FGK-type\n",
      "primaries with known metallicity. After training a deep learning (DL) neural\n",
      "network model on synthetic PHOENIX-ACES spectra, we used the internal feature\n",
      "representations together with those 14+6 stars with independent parameter\n",
      "measurements as a new input for the transfer process. We compare the derived\n",
      "stellar parameters of a small sample of M dwarfs kept out of the training phase\n",
      "with results from other methods in the literature. Assuming that temperatures\n",
      "from bolometric luminosities and interferometric radii and metallicities from\n",
      "FGK+M binaries are sufficiently accurate, DTL provides a higher accuracy than\n",
      "our previous state-of-the-art DL method (mean absolute differences improve by\n",
      "20 K for temperature and 0.2 dex for metallicity from DL to DTL when compared\n",
      "with reference values from interferometry and FGK+M binaries). Furthermore, the\n",
      "machine learning (internal) precision of DTL also improves as uncertainties are\n",
      "five times smaller on average. These results indicate that DTL is a robust tool\n",
      "for obtaining M-dwarf stellar parameters comparable to those obtained from\n",
      "independent estimations for well-known stars.\n",
      "\n",
      "Comment: None\n",
      "Primary category: astro-ph.SR\n",
      "All categories: astro-ph.SR, astro-ph.EP, astro-ph.IM\n",
      "URL: https://arxiv.org/pdf/2304.00224v1.pdf\n",
      "DOI: https://doi.org/10.1051/0004-6361/202243934\n",
      "Journal reference: A&A 673, A105 (2023)\n",
      "Submitted: 2023-04-01\n",
      "Last updated: 2023-04-01\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2304.14283v1\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "Distinguishing a planetary transit from false positives: a\n",
      "  Transformer-based classification for planetary transit signals\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "Helem Salinas, Karim Pichara, Rafael Brahm, Francisco Pérez-Galarce, Domingo Mery\n",
      "\n",
      "Abstract\n",
      "--------\n",
      "Current space-based missions, such as the Transiting Exoplanet Survey\n",
      "Satellite (TESS), provide a large database of light curves that must be\n",
      "analysed efficiently and systematically. In recent years, deep learning (DL)\n",
      "methods, particularly convolutional neural networks (CNN), have been used to\n",
      "classify transit signals of candidate exoplanets automatically. However, CNNs\n",
      "have some drawbacks; for example, they require many layers to capture\n",
      "dependencies on sequential data, such as light curves, making the network so\n",
      "large that it eventually becomes impractical. The self-attention mechanism is a\n",
      "DL technique that attempts to mimic the action of selectively focusing on some\n",
      "relevant things while ignoring others. Models, such as the Transformer\n",
      "architecture, were recently proposed for sequential data with successful\n",
      "results. Based on these successful models, we present a new architecture for\n",
      "the automatic classification of transit signals. Our proposed architecture is\n",
      "designed to capture the most significant features of a transit signal and\n",
      "stellar parameters through the self-attention mechanism. In addition to model\n",
      "prediction, we take advantage of attention map inspection, obtaining a more\n",
      "interpretable DL approach. Thus, we can identify the relevance of each element\n",
      "to differentiate a transit signal from false positives, simplifying the manual\n",
      "examination of candidates. We show that our architecture achieves competitive\n",
      "results concerning the CNNs applied for recognizing exoplanetary transit\n",
      "signals in data from the TESS telescope. Based on these results, we demonstrate\n",
      "that applying this state-of-the-art DL model to light curves can be a powerful\n",
      "technique for transit signal detection while offering a level of\n",
      "interpretability.\n",
      "\n",
      "Comment: None\n",
      "Primary category: astro-ph.EP\n",
      "All categories: astro-ph.EP, astro-ph.IM, cs.AI, cs.LG\n",
      "URL: https://arxiv.org/pdf/2304.14283v1.pdf\n",
      "DOI: https://doi.org/10.1093/mnras/stad1173\n",
      "Journal reference: Monthly Notices of the Royal Astronomical Society, 2023\n",
      "Submitted: 2023-04-27\n",
      "Last updated: 2023-04-27\n",
      "\n",
      "================================================\n",
      "arXiv.org e-Print 2305.02470v2\n",
      "================================================\n",
      "Title\n",
      "-----\n",
      "Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New\n",
      "  Exoplanets Using The Multiplicity Boost of ExoMiner\n",
      "\n",
      "Author(s)\n",
      "---------\n",
      "Hamed Valizadegan, Miguel J. S. Martinho, Jon M. Jenkins, Douglas A. Caldwell, Joseph D. Twicken, Stephen T. Bryson\n",
      "\n",
      "Abstract\n",
      "--------\n",
      "Most existing exoplanets are discovered using validation techniques rather\n",
      "than being confirmed by complementary observations. These techniques generate a\n",
      "score that is typically the probability of the transit signal being an\n",
      "exoplanet (y(x)=exoplanet) given some information related to that signal\n",
      "(represented by x). Except for the validation technique in Rowe et al. (2014)\n",
      "that uses multiplicity information to generate these probability scores, the\n",
      "existing validation techniques ignore the multiplicity boost information. In\n",
      "this work, we introduce a framework with the following premise: given an\n",
      "existing transit signal vetter (classifier), improve its performance using\n",
      "multiplicity information. We apply this framework to several existing\n",
      "classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et\n",
      "al. 2017), AstroNet (Shallue & Vanderburg 2018), ExoNet (Ansdel et al. 2018),\n",
      "GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to\n",
      "support our claim that this framework is able to improve the performance of a\n",
      "given classifier. We then use the proposed multiplicity boost framework for\n",
      "ExoMiner V1.2, which addresses some of the shortcomings of the original\n",
      "ExoMiner classifier (Valizadegan et al. 2022), and validate 69 new exoplanets\n",
      "for systems with multiple KOIs from the Kepler catalog.\n",
      "\n",
      "Comment: The paper is accepted for publication in the Astronomical Journal in\n",
      "  April 27th, 2023\n",
      "Primary category: astro-ph.EP\n",
      "All categories: astro-ph.EP, astro-ph.IM, cs.AI, cs.LG\n",
      "URL: https://arxiv.org/pdf/2305.02470v2.pdf\n",
      "DOI: https://doi.org/10.3847/1538-3881/acd344\n",
      "Journal reference: None\n",
      "Submitted: 2023-05-04\n",
      "Last updated: 2023-05-05\n"
     ]
    }
   ],
   "source": [
    "results = search.results(detail=\"high\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60ecd0",
   "metadata": {},
   "source": [
    "From here, if only one or two of the e-prints found by `Search` are of interest, they can be downloaded individually using either the URL to their .pdf files included in the `results` method, or by calling the `download` method on `eprint_from_arxiv_id` (as seen in `example_1.ipynb`). However, if all of the e-prints are of interest, they can be downloaded collectively using the `download_results` method. To learn more about the `download_results` method, we can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5c14f-4c57-4ed6-90ec-a011b669004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417643f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mpyxiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Method that downloads the search results as .pdf files to a\n",
       "specified local directory.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "save_directory : str\n",
       "    The directory to which the e-prints are saved.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Union[list[str], None]\n",
       "    If the search yields results, a list of the names of the\n",
       "    downloaded files is returned. If the search yields no results,\n",
       "    `None` is returned and a message to help with\n",
       "    troubleshooting is printed.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\rusla\\dropbox\\23-github\\projects\\chat-research\\pyxiv\\search.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyxiv.Search.download_results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6275257",
   "metadata": {},
   "source": [
    "For example, we can download all the e-prints found by `search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2408838b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading e-prints...\n",
      "[1/5] 'ExoplANNET: A deep learning algorithm to detect and identify planetary\n",
      "  signals in radial velocity data' (2303.09335v2)\n",
      "[2/5] 'A Catalogue of Exoplanet Atmospheric Retrieval Codes' (2303.12925v1)\n",
      "[3/5] 'The CARMENES search for exoplanets around M dwarfs -- A deep transfer\n",
      "  learning method to determine Teff and [M/H] of target stars' (2304.00224v1)\n",
      "[4/5] 'Distinguishing a planetary transit from false positives: a\n",
      "  Transformer-based classification for planetary transit signals' (2304.14283v1)\n",
      "[5/5] 'Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New\n",
      "  Exoplanets Using The Multiplicity Boost of ExoMiner' (2305.02470v2)\n",
      "Download complete! 5 e-prints (20.9 MiB) were downloaded in 1 min 29.4 sec and saved to ./papers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./papers/2303.09335v2.pdf',\n",
       " './papers/2303.12925v1.pdf',\n",
       " './papers/2304.00224v1.pdf',\n",
       " './papers/2304.14283v1.pdf',\n",
       " './papers/2305.02470v2.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.download_results(\"./papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c3c8a",
   "metadata": {},
   "source": [
    "If the path provided to `download_results` does not exist, it is created automatically. As well, note that the `download_results` method returns a list of the paths to the .pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ab2fd-a691-4830-8c53-b5fed544fac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
