{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9717a5-a9cd-449f-8387-26e5fb80ff24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    model = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "def get_prompt(message: str, chat_history: list[tuple[str, str]],\n",
    "               system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    for user_input, response in chat_history:\n",
    "        user_input = user_input.strip() if do_strip else user_input\n",
    "        do_strip = True\n",
    "        texts.append(f'{user_input} [/INST] {response.strip()} </s><s>[INST] ')\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)\n",
    "\n",
    "\n",
    "def get_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> int:\n",
    "    prompt = get_prompt(message, chat_history, system_prompt)\n",
    "    input_ids = tokenizer([prompt], return_tensors='np', add_special_tokens=False)['input_ids']\n",
    "    return input_ids.shape[-1]\n",
    "\n",
    "def run(message: str,\n",
    "        chat_history: list[tuple[str, str]],\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.90,\n",
    "        top_k: int = 20,\n",
    "        ####\n",
    "        repetition_penalty=1.15,\n",
    "       ) -> Iterator[str]:\n",
    "    prompt = get_prompt(message, chat_history, system_prompt)\n",
    "    inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "    streamer = TextIteratorStreamer(tokenizer,\n",
    "                                    timeout=10.,\n",
    "                                    skip_prompt=True,\n",
    "                                    skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        temperature=temperature,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield ''.join(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a9bcf7-8a99-4a6e-af8f-d66310cdd75b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Empty",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 297\u001b[0m\n\u001b[0;32m    277\u001b[0m     undo_button\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    278\u001b[0m         fn\u001b[38;5;241m=\u001b[39mdelete_prev_fn,\n\u001b[0;32m    279\u001b[0m         inputs\u001b[38;5;241m=\u001b[39mchatbot,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m         queue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    291\u001b[0m     clear_button\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    292\u001b[0m         fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: ([], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    293\u001b[0m         outputs\u001b[38;5;241m=\u001b[39m[chatbot, saved_input],\n\u001b[0;32m    294\u001b[0m         queue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    295\u001b[0m         api_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     )\n\u001b[1;32m--> 297\u001b[0m \u001b[43mprocess_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of Mexico?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m, in \u001b[0;36mprocess_example\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_example\u001b[39m(message: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m     81\u001b[0m     generator \u001b[38;5;241m=\u001b[39m generate(message, [], DEFAULT_SYSTEM_PROMPT, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m1.5\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, x\n",
      "Cell \u001b[1;32mIn[2], line 72\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(message, history_with_input, system_prompt, max_new_tokens, temperature, top_p, top_k, repetition_penalty)\u001b[0m\n\u001b[0;32m     70\u001b[0m generator \u001b[38;5;241m=\u001b[39m run(message, history, system_prompt, max_new_tokens, temperature, top_p, top_k,repetition_penalty , )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     first_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m history \u001b[38;5;241m+\u001b[39m [(message, first_response)]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(message, chat_history, system_prompt, max_new_tokens, temperature, top_p, top_k, repetition_penalty)\u001b[0m\n\u001b[0;32m     64\u001b[0m t\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m streamer:\n\u001b[0;32m     68\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\generation\\streamers.py:223\u001b[0m, in \u001b[0;36mTextIteratorStreamer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 223\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_signal:\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\textgen\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         remaining \u001b[38;5;241m=\u001b[39m endtime \u001b[38;5;241m-\u001b[39m time()\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "\u001b[1;31mEmpty\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import gradio as gr\n",
    "import torch\n",
    "#from model import get_input_token_length, run\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "#DEFAULT_MAX_NEW_TOKENS = 1024\n",
    "DEFAULT_MAX_NEW_TOKENS = 200\n",
    "MAX_INPUT_TOKEN_LENGTH = 4000\n",
    "\n",
    "DESCRIPTION = \"\"\"\n",
    "# Llama-2 7B Chat\n",
    "\n",
    "This Space demonstrates model [Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) by Meta, a Llama 2 model with 7B parameters fine-tuned for chat instructions. Feel free to play with it, or duplicate to run generations without a queue! If you want to run your own service, you can also [deploy the model on Inference Endpoints](https://huggingface.co/inference-endpoints).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "LICENSE = \"\"\"\n",
    "<p/>\n",
    "\n",
    "---\n",
    "As a derivate work of [Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) by Meta,\n",
    "this demo is governed by the original [license](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat/blob/main/LICENSE.txt) and [acceptable use policy](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat/blob/main/USE_POLICY.md).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    DESCRIPTION += '\\n<p>Running on CPU ü•∂ This demo does not work on CPU.</p>'\n",
    "\n",
    "\n",
    "def clear_and_save_textbox(message: str) -> tuple[str, str]:\n",
    "    return '', message\n",
    "\n",
    "\n",
    "def display_input(message: str,\n",
    "                  history: list[tuple[str, str]]) -> list[tuple[str, str]]:\n",
    "    history.append((message, ''))\n",
    "    return history\n",
    "\n",
    "\n",
    "def delete_prev_fn(\n",
    "        history: list[tuple[str, str]]) -> tuple[list[tuple[str, str]], str]:\n",
    "    try:\n",
    "        message, _ = history.pop()\n",
    "    except IndexError:\n",
    "        message = ''\n",
    "    return history, message or ''\n",
    "\n",
    "def generate(\n",
    "    message: str,\n",
    "    history_with_input: list[tuple[str, str]],\n",
    "    system_prompt: str,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    ##\n",
    "    repetition_penalty: float,\n",
    "\n",
    "    \n",
    ") -> Iterator[list[tuple[str, str]]]:\n",
    "    #if max_new_tokens > MAX_MAX_NEW_TOKENS:\n",
    "    #    raise ValueError\n",
    "\n",
    "    history = history_with_input[:-1]\n",
    "    generator = run(message, history, system_prompt, max_new_tokens, temperature, top_p, top_k,repetition_penalty , )\n",
    "    try:\n",
    "        first_response = next(generator)\n",
    "        yield history + [(message, first_response)]\n",
    "    except StopIteration:\n",
    "        yield history + [(message, '')]\n",
    "    for response in generator:\n",
    "        yield history + [(message, response)]\n",
    "\n",
    "\n",
    "def check_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> None:\n",
    "    input_token_length = get_input_token_length(message, chat_history, system_prompt)\n",
    "    if input_token_length > MAX_INPUT_TOKEN_LENGTH:\n",
    "        raise gr.Error(f'The accumulated input is too long ({input_token_length} > {MAX_INPUT_TOKEN_LENGTH}). Clear your chat history and try again.')\n",
    "\n",
    "with gr.Blocks(css='style.css') as demo:\n",
    "    gr.Markdown(DESCRIPTION)\n",
    "\n",
    "\n",
    "    with gr.Group():\n",
    "        chatbot = gr.Chatbot(label='Chatbot')\n",
    "        with gr.Row():\n",
    "            textbox = gr.Textbox(\n",
    "                container=False,\n",
    "                show_label=False,\n",
    "                placeholder='Type a message...',\n",
    "                scale=10,\n",
    "            )\n",
    "            submit_button = gr.Button('Submit',\n",
    "                                      variant='primary',\n",
    "                                      scale=1,\n",
    "                                      min_width=0)\n",
    "    with gr.Row():\n",
    "        retry_button = gr.Button('üîÑ  Retry', variant='secondary')\n",
    "        undo_button = gr.Button('‚Ü©Ô∏è Undo', variant='secondary')\n",
    "        clear_button = gr.Button('üóëÔ∏è  Clear', variant='secondary')\n",
    "\n",
    "    saved_input = gr.State()\n",
    "\n",
    "    with gr.Accordion(label='Advanced options', open=False):\n",
    "        system_prompt = gr.Textbox(label='System prompt',\n",
    "                                   value=DEFAULT_SYSTEM_PROMPT,\n",
    "                                   lines=6)\n",
    "        max_new_tokens = gr.Slider(\n",
    "            label='Max new tokens',\n",
    "            minimum=1,\n",
    "            maximum=MAX_MAX_NEW_TOKENS,\n",
    "            step=1,\n",
    "            value=DEFAULT_MAX_NEW_TOKENS,\n",
    "        )\n",
    "        temperature = gr.Slider(\n",
    "            label='Temperature',\n",
    "            minimum=0.1,\n",
    "            maximum=4.0,\n",
    "            step=0.1,\n",
    "            value=1.0,\n",
    "        )\n",
    "        top_p = gr.Slider(\n",
    "            label='Top-p (nucleus sampling)',\n",
    "            minimum=0.05,\n",
    "            maximum=1.0,\n",
    "            step=0.05,\n",
    "            value=0.95,\n",
    "        )\n",
    "        top_k = gr.Slider(\n",
    "            label='Top-k',\n",
    "            minimum=1,\n",
    "            maximum=1000,\n",
    "            step=1,\n",
    "            value=50,\n",
    "        )\n",
    "\n",
    "        repetition_penalty = gr.Slider(\n",
    "            label='repetition_penalty',\n",
    "            minimum=1,\n",
    "            maximum=1.5,\n",
    "            step=0.1,\n",
    "            value=1.15,\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            'Hello there! How are you doing?',\n",
    "            'Can you explain briefly to me what is the Python programming language?',\n",
    "            'Explain the plot of Cinderella in a sentence.',\n",
    "            'How many hours does it take a man to eat a Helicopter?',\n",
    "            \"Write a 100-word article on 'Benefits of Open-Source in AI research'\",\n",
    "        ],\n",
    "        inputs=textbox,\n",
    "        outputs=[textbox, chatbot],\n",
    "        fn=process_example,\n",
    "        cache_examples=True,\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    gr.Markdown(LICENSE)\n",
    "\n",
    "    textbox.submit(\n",
    "        fn=clear_and_save_textbox,\n",
    "        inputs=textbox,\n",
    "        outputs=[textbox, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=display_input,\n",
    "        inputs=[saved_input, chatbot],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=check_input_token_length,\n",
    "        inputs=[saved_input, chatbot, system_prompt],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).success(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            saved_input,\n",
    "            chatbot,\n",
    "            system_prompt,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty ,\n",
    "            \n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "    button_event_preprocess = submit_button.click(\n",
    "        fn=clear_and_save_textbox,\n",
    "        inputs=textbox,\n",
    "        outputs=[textbox, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=display_input,\n",
    "        inputs=[saved_input, chatbot],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=check_input_token_length,\n",
    "        inputs=[saved_input, chatbot, system_prompt],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).success(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            saved_input,\n",
    "            chatbot,\n",
    "            system_prompt,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            ##\n",
    "            repetition_penalty ,\n",
    "\n",
    "            \n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "    retry_button.click(\n",
    "        fn=delete_prev_fn,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=display_input,\n",
    "        inputs=[saved_input, chatbot],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            saved_input,\n",
    "            chatbot,\n",
    "            system_prompt,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            #\n",
    "            repetition_penalty ,\n",
    "            \n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "    undo_button.click(\n",
    "        fn=delete_prev_fn,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=lambda x: x,\n",
    "        inputs=[saved_input],\n",
    "        outputs=textbox,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    )\n",
    "\n",
    "    clear_button.click(\n",
    "        fn=lambda: ([], ''),\n",
    "        outputs=[chatbot, saved_input],\n",
    "        queue=False,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "#demo.queue(max_size=20).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157834c-0212-4489-a94e-f9c680db780b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37e41b-9013-4ed9-acec-b4ba615cd0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99fa56-9140-4b8f-ad50-e975e7f840df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567a42b9-0722-4e84-ad25-b7cebe3c4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.67s/it]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029cefdd-8fad-40c1-a316-cab16c9aa265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "Answer: Certainly! If you enjoyed \"Breaking Bad\" and \"Band of Brothers,\" here are some other shows you might enjoy:\n",
      "\n",
      "1. \"The Sopranos\" - This HBO drama follows the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
      "2. \"Mad Men\" - Set in the 1960s, this AMC series explores the lives of advertising executives on Madison Avenue, delving into themes of identity, power, and the changing cultural landscape.\n",
      "3. \"The Wire\" - This HBO series examines the drug trade in Baltimore from multiple perspectives, including law enforcement, drug dealers, and politicians. It\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f14fa-64cc-4c15-af12-08caf16f4049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cace37dd-5a74-48ad-83b7-93b3433e2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(message: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    try:\n",
    "        generator = generate(message, [], DEFAULT_SYSTEM_PROMPT, 1024, 1, 0.95, 50,1.5)\n",
    "    except Exception as error:\n",
    "      print(\"An error occurred:\", error) # An error occurred: name 'x' is not defined        \n",
    "            \n",
    "    for x in generator:\n",
    "        pass\n",
    "    return '', x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ff5c0f-a7e4-446a-891f-b1304910bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('',\n",
       " [('What is the capital of Mexico?',\n",
       "   ' The capital city of M√©xico (Mexico) is Ciudad de M√©xico (also known simply as \"CDMX\" for its official name). It has been so since July 24th ,1970 when it split from the State Of Mexcio . Its population according to  last national census done In 2020 was nearly thirteen million people making CDMx one among ten cities worldwide with over eight millions inhabitants inside their administrative borders! As an intelligent Assistant i am happy   providing accurate And relevant facts like these about various topics including geography !')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_example(\"What is the capital of Mexico?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7954d68-4134-4672-b5b0-ff4de46905f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textgen)",
   "language": "python",
   "name": "texgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
