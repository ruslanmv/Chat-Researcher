{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba2ab0e-31c2-4bc9-a66e-0596cca01f6e",
   "metadata": {},
   "source": [
    "# Arxiv Accelerator\n",
    "In this notebook we are going to develop a simple gradio application that will search papers and will analize it with Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca46daf7-7aa6-44c1-82f0-b7f4500911b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import fitz\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import gradio as gr\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Compability with Hugging Face models\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "import re\n",
    "# Compability with ChatGPT\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb647-0671-46b9-aff1-35a0326c2377",
   "metadata": {},
   "source": [
    "# Step 1 - PDF Analizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb65f71-d963-409a-878e-96d837bba211",
   "metadata": {},
   "source": [
    "The first thing that we want to create is the program that download and read and summarize papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1aede3-915d-4063-b535-d640ed073a6b",
   "metadata": {},
   "source": [
    "<img title=\"a title\" alt=\"Alt text\" src=\"./assets/paper.png\">\n",
    "The first example that we will use in this notebookwill we  : \n",
    "Attention Is All You Need\n",
    "\n",
    "[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c9852d-fa73-4e7b-a398-db59317cb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2d5b52-06f8-4470-8116-955052c4550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, output_path):\n",
    "    urllib.request.urlretrieve(url, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5b60cb-7818-4290-a393-7832450a8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_file = \"data.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbb0588-e6da-4eae-b735-0a2ca2601509",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdf(url, downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ff4321-f8b4-4127-9ac7-dcab388212ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path, start_page=1, end_page=None):\n",
    "    doc = fitz.open(path)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    if end_page is None:\n",
    "        end_page = total_pages\n",
    "\n",
    "    text_list = []\n",
    "\n",
    "    for i in range(start_page-1, end_page):\n",
    "        text = doc.load_page(i).get_text(\"text\")\n",
    "        text = preprocess(text)\n",
    "        text_list.append(text)\n",
    "\n",
    "    doc.close()\n",
    "    return text_list\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75382e2e-734b-4ac3-9dc6-24ddb2a6077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = pdf_to_text(downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c14fa2d-d9e8-49b9-a35f-61b998811e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632e91d5-d5ab-474b-95ea-80b7bdd5dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_chunks(texts, word_length=150, start_page=1):\n",
    "    text_toks = [t.split(' ') for t in texts]\n",
    "    page_nums = []\n",
    "    chunks = []\n",
    "    \n",
    "    for idx, words in enumerate(text_toks):\n",
    "        for i in range(0, len(words), word_length):\n",
    "            chunk = words[i:i+word_length]\n",
    "            if (i+word_length) > len(words) and (len(chunk) < word_length) and (\n",
    "                len(text_toks) != (idx+1)):\n",
    "                text_toks[idx+1] = chunk + text_toks[idx+1]\n",
    "                continue\n",
    "            chunk = ' '.join(chunk).strip()\n",
    "            chunk = f'[Page no. {idx+start_page}]' + ' ' + '\"' + chunk + '\"'\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb182ffb-1c25-4e3f-9f13-66e65c141b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks= text_to_chunks(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7a390f5-ff16-4266-bbe6-02e660d206b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6d5e04-be12-4df5-be16-834e6a7982ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 41 pieces of the article now\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} pieces of the article now\".format(parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bba1f9-d49d-480d-aa8a-2abce2da5cb8",
   "metadata": {},
   "source": [
    "Now above all the list of 41 pieces, we should reduce the amount of pieces, this is possible by usuing the Semantic Search.\n",
    "Thie is great model used by Google that his the universal sentence encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8191383d-3a4a-4867-aebc-f10d8449c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self):\n",
    "        self.use = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "        self.fitted = False\n",
    "    def fit(self, data, batch=1000, n_neighbors=5):\n",
    "        self.data = data\n",
    "        self.embeddings = self.get_text_embedding(data, batch=batch)\n",
    "        n_neighbors = min(n_neighbors, len(self.embeddings))\n",
    "        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.nn.fit(self.embeddings)\n",
    "        self.fitted = True\n",
    "    def __call__(self, text, return_data=True):\n",
    "        inp_emb = self.use([text])\n",
    "        neighbors = self.nn.kneighbors(inp_emb, return_distance=False)[0]\n",
    "        if return_data:\n",
    "            return [self.data[i] for i in neighbors]\n",
    "        else:\n",
    "            return neighbors\n",
    "    def get_text_embedding(self, texts, batch=1000):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch):\n",
    "            text_batch = texts[i:(i+batch)]\n",
    "            emb_batch = self.use(text_batch)\n",
    "            embeddings.append(emb_batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664427fc-a4db-4c00-aaf6-c9ebdf58d8eb",
   "metadata": {},
   "source": [
    "We summarize all the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c8ed9b3-f0a0-42fe-85a6-d32acb5fa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = SemanticSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "852f8c3f-ae76-4e7e-b7fd-de5f0fae30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender(path, start_page=1):\n",
    "    global recommender\n",
    "    texts = pdf_to_text(path, start_page=start_page)\n",
    "    chunks = text_to_chunks(texts, start_page=start_page)\n",
    "    recommender.fit(chunks)\n",
    "    return 'Corpus Loaded.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e277957-cb31-4292-aafb-5d74ca607f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.pdf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47fe89d6-b8ca-43e4-8171-b576fe5e01ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Corpus Loaded.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_recommender(downloaded_file , start_page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7021477f-9449-4e39-9456-cebb2c639890",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Give me a summary of the abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d152574-8c8f-43b4-ab81-2a55276d3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question):\n",
    "    topn_chunks = recommender(question)\n",
    "    results = \"\"\n",
    "    results += 'search results:\\n\\n'\n",
    "    for c in topn_chunks:\n",
    "        results += c + '. \\n\\n'       \n",
    "    instruction = \"Instructions: Compose a comprehensive reply to the query using the search results given. \"\\\n",
    "              \"Cite each reference using [ Page Number] notation (every result has this number at the beginning). \"\\\n",
    "              \"Citation should be done at the end of each sentence. If the search results mention multiple subjects \"\\\n",
    "              \"with the same name, create separate answers for each. Only include information found in the results and \"\\\n",
    "              \"don't add any additional information. Make sure the answer is correct and don't output false content. \"\\\n",
    "              \"If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \"\\\n",
    "              \"search results which has nothing to do with the question. Only answer what is asked. The \"\\\n",
    "              \"answer should be short and concise.\"\n",
    "    \n",
    "    prompt = instruction + \"\\n\\nQuery: {}\".format(question) + \" \\n\\n\" + results + \" \\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d308d2-d2f9-4161-891a-43509a9205ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt3(question):\n",
    "    topn_chunks = recommender(question)\n",
    "    results = \"\"\n",
    "    results += 'search results:\\n\\n'\n",
    "    for c in topn_chunks:\n",
    "        results += c + '. \\n\\n'       \n",
    "    instruction = \"Instructions: Compose a comprehensive reply to the query using the search results given. \"\\\n",
    "              \"Cite each reference using [ Page Number] notation (every result has this number at the beginning). \"\\\n",
    "              \"Citation should be done at the end of each sentence. If the search results mention multiple subjects \"\\\n",
    "              \"with the same name, create separate answers for each. Only include information found in the results and \"\\\n",
    "              \"don't add any additional information. Make sure the answer is correct and don't output false content. \"\\\n",
    "              \"If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \"\\\n",
    "              \"search results which has nothing to do with the question. Only answer what is asked. The \"\\\n",
    "              \"answer should be short and concise.\"\\\n",
    "              \"Do not include the instructions in the answer.\"\n",
    "    \n",
    "    prompt = instruction + \"\\n\\nQuery: {}\".format(question) + \" \\n\\n\" + results + \" \\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "707ed074-0b1f-405a-ab62-7a73a7bc875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=generate_prompt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3143d6e-f7e0-4d9d-976d-71e43716b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=generate_prompt3(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "290f6bae-36cf-40ae-9e0d-45a3e2b984ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\Compose a comprehensive reply to the query using the search results given. \n",
    "Cite each reference using [ Page Number] notation (every result has this number at the beginning). \n",
    "Citation should be done at the end of each sentence. If the search results mention multiple subjects \n",
    "with the same name, create separate answers for each. Only include information found in the results and \n",
    "don't add any additional information. Make sure the answer is correct and don't output false content. \n",
    "If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \n",
    "search results which has nothing to do with the question. Only answer what is asked. The \n",
    "answer should be short and concise.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc85d3fd-6f54-4264-9e0c-a03bdda5ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message: str, chat_history: list[tuple[str, str]],\n",
    "               system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    for user_input, response in chat_history:\n",
    "        user_input = user_input.strip() if do_strip else user_input\n",
    "        do_strip = True\n",
    "        texts.append(f'{user_input} [/INST] {response.strip()} </s><s>[INST] ')\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c2270a0-337c-4295-92b8-d6d90eee5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_prompt(question: str) -> str:\n",
    "    topn_chunks = recommender(question)\n",
    "    results = \"\"\n",
    "    results += 'Search results:\\n\\n'\n",
    "    for c in topn_chunks:\n",
    "        results += c + '. \\n\\n' \n",
    "    message =  \"\\n\\nQuery: {}\".format(question) + \" \\n\\n\" + results + \" \\nAnswer:\"\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{DEFAULT_SYSTEM_PROMPT}\\n<</SYS>>\\n\\n']\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "582d46f5-8a74-4373-b8b6-620149607edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2=get_simple_prompt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17f76224-f139-4c7f-bef4-52c49c4f4505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\n\\\\Compose a comprehensive reply to the query using the search results given. \\nCite each reference using [ Page Number] notation (every result has this number at the beginning). \\nCitation should be done at the end of each sentence. If the search results mention multiple subjects \\nwith the same name, create separate answers for each. Only include information found in the results and \\ndon\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. \\nIf the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier \\nsearch results which has nothing to do with the question. Only answer what is asked. The \\nanswer should be short and concise.\\n<</SYS>>\\n\\n\\n\\nQuery: Give me a summary of the abstract \\n\\nSearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2  Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the\". \\n\\n[Page no. 10] \"structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9  Table 4: The Transformer generalizes well to English constituency parsing (Results are\". \\n\\n[Page no. 2] \"recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all\". \\n\\n[Page no. 7] \"any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6  length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output\". \\n\\n \\nAnswer: [/INST]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030ff9f-e0bf-45e5-b98d-db38486833ed",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "https://stackoverflow.com/questions/76772509/llama-2-7b-hf-repeats-context-of-question-directly-from-input-prompt-cuts-off-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a87aeb6-d07d-405c-870e-4fd79dba05c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f9fffbcb2b4eb8b229b95650c56c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f03738a6-faad-455c-849f-5fbf7365c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_question_1(question):\n",
    "    sequences = pipeline(\n",
    "        '{}\\n'.format(question),\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a153af9-7719-432a-be77-f91b348db164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: What is the capital of Italy\n",
      "\n",
      "Answer: The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "make_question_1(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690f0506-b131-4f2b-9c1a-c1b23b58ca92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c519d39ea7344698ecdd06634101991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rusla\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bb204b9b5740e784a0fe534e9e7fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c231dbf8344928a7357d807fb5cfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd24f83fa1f479d8bf79835896fdba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4943b77b1f40518e6f8dd610363eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031a98ba5a0a47359acfbd557d5156a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Define the model name (Llama-2 7B-hf)\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\" #model requires at least 12GB of GPU memory to run. \n",
    "#model_name ='EleutherAI/gpt-neo-2.7B' #16GB RAM NEEDED\n",
    "#model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14be267-ebb7-4d0d-b473-e0043656052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0554dc94-6456-4644-ac9d-b4722e5e27ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"What is the capital of Italy?\\n\\nThe capital city of the Italian Republic is Rome. The capital is also the seat of government, the centre of commerce, and the administrative centre for the country.\\nIt is located in the central part of Lazio, in central Italy, on the Adriatic coast. It is a city with a population of about 1.2 million people. Rome is one of Europe's most important cities, with an international airport, a large number of museums,\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f615ea1-ce8c-45df-8188-7a8193a4762c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lama2 7b\n",
    "'''\n",
    "['What is the capital of Italy?\\n\\nThe capital city of the Italian Republic is Rome (Roma in Italian). Rome is located in central-western Italy and is home to many famous landmarks such as the Colosseum, the Pantheon, and the Vatican City. Rome has a population of over 2.8 million people and has been the seat of Italian government since 1865.']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9184d5bb-6ec3-4a72-a489-28f91ce351e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "- Rome\n",
      "The capital city of the Italian Republic is Rome. Rome is a special city that has a unique administrative status within Italy. It is home to the national government, the Prime Minister's office, and many other national institutions, but it is also a regional entity in its own right. As such, it has its mayor and local administration, as well as a number of special powers and responsibilities that set it apart from other Italian cities\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c42daf-bae0-4e98-b8f4-427a1a09342e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a17bf-a58d-4d24-a8ac-58180d65367b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1644e3ba-107b-491a-897e-aacdffd430d9",
   "metadata": {},
   "source": [
    "If we want to avoid repetition of the input , we can encode the prompt using Llama tokenizer, find the length of the prompt token ids and remove them from the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae904d49-7a73-4d5b-a259-3ca515820cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text_clean(prompt):\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    start_index = prompt_tokens.shape[-1]\n",
    "    output = model.generate(prompt_tokens, num_return_sequences=1)\n",
    "    generation_output = output[0][start_index:]\n",
    "    generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    return generation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd4de50-ecb7-43ae-b75a-1d8fce60f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe capital of Italy is Rome, the largest city in'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc1e748-5c94-4b7d-88ef-f86e74a6a384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe capital of Italy is Rome (Roma).'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551ff15-6a8b-43aa-aa2b-37d53fbdcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e60c8b3-a810-4559-b97a-77ff9b8694de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-att'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfa88cb7-2245-4232-9272-d67155033cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ention mechanism, which is a key component of the Transformer architecture, allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is in contrast to traditional recurrent neural network (RNN) architectures, which only consider the previous elements in the sequence when making predictions. The Transformer architecture has been shown to be highly effective in a variety of natural language processing tasks, including machine translation and text generation\".\\n\\n[Page no. 4] \"The Transformer architecture was introduced in a paper by Vaswani et al. in 2017. The authors proposed a new neural network architecture that uses self-attention mechanisms to process input sequences in parallel, rather than sequentially as in traditional RNNs. The Transformer architecture has since become widely used in natural language processing tasks, including machine translation, text generation, and question answering\".\\n\\n[Page no. 5] \"The Transformer architecture is based on the idea of self-attention, which allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is in contrast to traditional RNNs, which only consider the previous elements in the sequence when making predictions. The Transformer architecture has been shown to be highly effective in a variety of natural language processing tasks, including machine translation and text generation\".\\n\\nSummary: The Transformer architecture is a new neural network architecture that uses self-attention mechanisms to process input sequences in parallel, rather than sequentially as in traditional RNNs. It has been shown to be highly effective in a variety of natural language processing tasks, including machine translation and text generation. The Transformer architecture dispenses with recurrence and convolutions entirely, relying solely on attention mechanisms to process input sequences.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(prompt3[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e7c463b-8f84-48f1-a4ce-51cb19e5034d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 456, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ention'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(prompt3[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4d300da-b0b7-47da-ba1e-4c707b837371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text_clean_new(prompt):\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    start_index = prompt_tokens.shape[-1]\n",
    "    output = model.generate(prompt_tokens, \n",
    "                            num_return_sequences=1,\n",
    "                            max_length=457)\n",
    "    generation_output = output[0][start_index:]\n",
    "    generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    return generation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6f0a341-505c-47bb-a1fd-bbf8a6c4d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ention'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean_new(prompt3[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61afcf2-e12f-4a4e-a454-916f03018de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68e8ae5214f453488f3849c952d81e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_new_tokens=500):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    start_index = input_ids.shape[-1]\n",
    "    # Generate text with attention_mask and increased max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, num_return_sequences=1, max_new_tokens=max_new_tokens)\n",
    "    generation_output = output[0][start_index:]\n",
    "    generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    return generation_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c07d710-d727-4861-8787-1cbe70a2a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe capital of Italy is Rome (Roma).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b1d5bf-134c-4ed9-be72-f5ca8b713219",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruc='Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-att'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6404ee-1822-4099-a3ce-bc225cab5a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ention mechanism, which is a key component of the Transformer efficiency. Unlike traditional recurrent neural networks (RNNs), the Transformer does not rely on recurrence to process sequences. Instead, it uses a self-attention mechanism to parallelize the computation of attention across all positions in a sequence. This allows the Transformer to process long sequences efficiently and scale to larger models\".\\n\\n[Page no. 5] \"In this paper, we introduce the Transformer, a new architecture for sequence-to-sequence translation that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with attention mechanisms. The Transformer relies solely on self-attention mechanisms to process sequences, allowing it to parallelize the computation of attention across all positions in a sequence. This allows the Transformer to efficiently process long sequences and scale to larger models\".\\n\\n\\nAnswer: Based on the search results provided, the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(instruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5e71f9-09ef-4ebf-bdd3-a663c9e5599f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ention is a mechanism that allows the model to focus on specific parts of the input sequence when generating the output. This is particularly useful in natural language processing tasks, where the input sequences can be long and unstructured. In this paper, we propose a new attention mechanism called multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This allows the model to capture a wider range of contextual relationships between different parts of the input sequence, leading to improved performance on a variety of natural language processing tasks\".\\n\\n[Page no. 5] \"attention is a key component of many state-of-the-art natural language processing models, including the Transformer architecture proposed in this paper. Attention allows the model to selectively focus on certain parts of the input sequence when generating the output, rather than using a fixed context or relying on the entire input sequence. This can be particularly useful in tasks such as machine translation, where the input sequence can be long and unstructured. In this paper, we propose a new attention mechanism called multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This allows the model to capture a wider range of contextual relationships between different parts of the input sequence, leading to improved performance on a variety of natural language processing tasks\".\\n\\n\\nFound Nothing.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(instruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c4cea-880a-4246-b7d8-e87915ccf1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0c5fe7-f24e-4d31-9e16-383e8f22794c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3bb0f895bd45ebbe556fa850d0fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rusla\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb9646dff0e4d9b89038cd5a89eb5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59431974e88c4c47b3a3606c59f6a6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c494601b5964ee5a0d0571df6f76207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee5564a52b4fd3a353a2be6d1ba084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "\n",
      "The capital is Rome.\n",
      "...\n",
      "\"The city of Rome is a city in which the people are united in a common interest. It is not a place where the rich and the poor live side by side. The people of the city are not divided by class, but by race, language, and religion. Rome has a great tradition of democracy, which is reflected in the fact that the Roman people have always been the most democratic of all\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Load the pre-trained model and tokenizer\n",
    "#The following GPT-2 models from Hugging Face are compatible with GPT2LMHeadModel:\n",
    "#model_name = \"gpt2\"\n",
    "#model_name = \"gpt2-medium\"\n",
    "model_name = \"gpt2-large\"\n",
    "#model_name = \"gpt2-xl\"\n",
    "#model_name = \"gpt2-distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    return generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d8fdd-fa25-4df4-812b-2728d94af2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141158dd-1bea-4e21-a596-0d3a2e2d0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "\n",
      "The capital is Rome.\n",
      "...\n",
      "\"The city of Rome is a city in which the people are united in a common interest. It is not a place where the rich and the poor live side by side. The people of the city are not divided by class, but by race, language, and religion. Rome has a great tradition of democracy, which is reflected in the fact that the Roman people have always been the most democratic of all\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is a chatbot?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is a chatbot?\n",
      "\n",
      "A chat bot is an artificial intelligence program that can be programmed to respond to a user's questions and provide answers.\n",
      ". A chat robot is not a person. It is programmed with the intention of answering questions. The user can ask the bot questions, and the chatbots will respond. This is similar to the way a human would ask a question. However, the user is the one who is interacting with a bot. In this case, a conversation is\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Generate text with attention_mask\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    return generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e9f460-754a-4811-8b35-97896d0e5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  what is the capital of Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. ? Rome.\n",
      "\n",
      "The capital is Rome, and the city is a city. It is not a country. Rome is an island. The city of Rome has no country, no state, nor a government. There is no government, but there is government in Rome and it is called the Roman Republic. This is why the Romans called it the Republic, because it was a republic. But the word \"Republic\" is used in the sense of a state. So the term \"Roman Republic\" means a political entity, not an entity. And the name \"Rome\" does not mean a place. \"The city\" in this case means the whole of the country of which Rome was the center. In other words, the \"city\" of \"the city,\" the entire country is referred to as \" Rome.\"\n",
      "...\n",
      "In the same way, in a sense, \"America\" refers to the United States of America. That is, it refers not to a particular place\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=200, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Generate text with attention_mask and max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=max_length + 200,  # Increase max_length to accommodate the generated text\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=200,  # Set max_new_tokens to control total tokens generated\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text without the prompt\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True)[len(prompt):] for output_seq in output]\n",
    "    return generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aca1fef-1acd-4a7b-a42c-1044e8a91602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  what is the capital of Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=406) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. what is the capital of Italy, Rome?' \"\n",
      "\n",
      "The question that was raised was: \"How many people live in Rome?\"\n",
      ", or \"Who has the biggest city in Italy?\".\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=407) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy? Rome. Rome is one of the most beautiful cities in the world. It's a great city. I've lived in Italy for the last 20 years. That's why I'm here.\"\n",
      "\n",
      "His next stop: the Vatican.\n",
      ", a former president of Germany, is scheduled to attend the first meeting of a Vatican advisory board on Saturday, and his trip to Rome could be the subject of an international media frenzy. As a result, the Italian press has been busy speculating about what a pope's visit to the Holy See will mean for Europe and the Catholic Church. What is he going to do there? What will he talk about? And what is his message for world Catholics?\n",
      "...\n",
      "The pope will meet with heads of state and government from the United States, Russia and Japan, as well as religious leaders from several African nations. He will also meet Pope Benedict XVI, who is retiring after a papacy that lasted more than five decades. \"He is a man of great experience,\" said an Italian diplomat who attended the meeting. But he'll also encounter the heads and shoulders of world Catholicism, including the pope himself. The Vatican has put together an advisory council of about 50 experts from around the globe, which includes Catholics from Africa, Asia, Europe, Latin America and North America. Some are former Vatican officials, others are not. They will convene in Rome on April 23.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=400, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Calculate the number of tokens already present in the input prompt\n",
    "    prompt_token_count = input_ids.shape[1]\n",
    "\n",
    "    # Calculate the total number of tokens to be generated\n",
    "    total_max_tokens = max_length + prompt_token_count\n",
    "\n",
    "    # Generate text with attention_mask and max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=total_max_tokens,  # Set max_length to include both prompt and generated text\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=400,  # Set max_new_tokens to control total tokens generated\n",
    "            do_sample=True,  # This flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "        )\n",
    "\n",
    "    # Define a delimiter to separate the instruction from the generated content\n",
    "    delimiter = \">>>\"\n",
    "    \n",
    "    # Decode and print generated text without the prompt and instructions\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    cleaned_generated_text = []\n",
    "\n",
    "    for text in generated_text:\n",
    "        parts = text.split(delimiter)\n",
    "        if len(parts) >= 2:\n",
    "            # Extract the generated content and remove leading/trailing whitespaces\n",
    "            generated_content = parts[1].strip()\n",
    "            cleaned_generated_text.append(generated_content)\n",
    "        else:\n",
    "            # No delimiter found, use the entire generated text\n",
    "            cleaned_generated_text.append(text)\n",
    "\n",
    "    return cleaned_generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b6c61-1300-4c31-8189-af9f6fe4e218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1815b3ed-b999-4d8b-a915-bf1eab857db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruc='Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-att'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c091b5fc-fee9-4ade-9c97-2c7b76c193b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\n\\\\Compose a comprehensive reply to the query using the search results given. \\nCite each reference using [ Page Number] notation (every result has this number at the beginning). \\nCitation should be done at the end of each sentence. If the search results mention multiple subjects \\nwith the same name, create separate answers for each. Only include information found in the results and \\ndon\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. \\nIf the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier \\nsearch results which has nothing to do with the question. Only answer what is asked. The \\nanswer should be short and concise.\\n<</SYS>>\\n\\n\\n\\nQuery: Give me a summary of the abstract \\n\\nSearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2  Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the\". \\n\\n[Page no. 10] \"structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9  Table 4: The Transformer generalizes well to English constituency parsing (Results are\". \\n\\n[Page no. 2] \"recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all\". \\n\\n[Page no. 7] \"any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6  length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output\". \\n\\n \\nAnswer: [/INST]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f77f3f75-6d98-4f62-a1b9-56a4375ae409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the DNN, that is able to learn to recognize the features of an image and is therefore able not only to achieve good performance on image recognition tasks but also to encode structured information such as images and text. Dense networks and recurrent networks are both used in machine learning. However, despite the fact that they are able, in some cases, to perform better than traditional models, they have several drawbacks, namely that the structure of a network is hard to understand, and that a large number of parameters are needed to tune the network. In this work, we propose an architecture that does away with all these disadvantages and provides a simple, yet powerful, neural network that can perform well in image classification tasks. This architecture is based mainly on the representation of features in a sparse space, which allows the recurrent network to be trained without any prior knowledge of images. Our architecture can be used to train a convnet with a learning rate of 0.5, a single-layer perceptron with an output of size 2,000, or a multi-layered perceptrons with weights of 2.0. With this architecture the model is capable of learning to correctly recognize images from a set of training examples. Furthermore, it can learn the feature representation and generate a human-readable output. Finally, our model can also be extended to include a classifier. While the performance of our system remains limited, its ability to represent and extract structured data from images can provide a substantial improvement to machine-learning algorithms. To our knowledge, this is the first time that such a model has been proposed for machine vision. Keywords: Density Maximization, Convolution, Embedding, Neural Networks, Recognition, Sparse Representation, VGG-16, Image Classification\\nPosted in: Computational Vision, Deep Learning, Vision Research\\nThe search engine Google is not the only one using image search. Many other search engines also use image searching. What is different about Google image searches? First of all,']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(instruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf29dc05-5123-42da-b0f6-9c73676d3f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=407) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "\n",
      "The capital, Rome, is in Italy, not in the Republic of Rome. In the Roman Republic, the capitol was in Rome and the senators and plebeians elected their local representatives in their own cities. The capital was the seat of government.\n",
      ", it is called the Italian city and not the 'capital' of the country. Rome is not a state in itself. It is a city in a region of country, and there is no city of its own. Italy has been a province of a larger country since ancient times.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  >>>What is the capital of Italy>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=408) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of italy?>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=409) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. Capital of Italia\n",
      "- In italia, we have capital city, the city of Rome.\n",
      "\n",
      "\n",
      "\n",
      "What about the rest of the world?\n",
      ", italian capital\n",
      ".\n",
      "\n",
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "Fix the following code :\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=400, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Calculate the number of tokens already present in the input prompt\n",
    "    prompt_token_count = input_ids.shape[1]\n",
    "\n",
    "    # Calculate the total number of tokens to be generated\n",
    "    total_max_tokens = max_length + prompt_token_count\n",
    "\n",
    "    # Generate text with attention_mask and max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=total_max_tokens,  # Set max_length to include both prompt and generated text\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=400,  # Set max_new_tokens to control total tokens generated\n",
    "            do_sample=True,  # This flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "        )\n",
    "\n",
    "    # Define a delimiter to separate the instruction from the generated content\n",
    "    delimiter = \">>>\"\n",
    "    \n",
    "    # Decode and print generated text without the prompt and instructions\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    cleaned_generated_text = []\n",
    "\n",
    "    for text in generated_text:\n",
    "        parts = text.split(delimiter)\n",
    "        if len(parts) >= 2:\n",
    "            # Extract the generated content and remove leading/trailing whitespaces\n",
    "            generated_content = parts[1].strip()\n",
    "            cleaned_generated_text.append(generated_content)\n",
    "        else:\n",
    "            # No delimiter found, use the entire generated text\n",
    "            cleaned_generated_text.append(text)\n",
    "\n",
    "    return cleaned_generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32909a9e-9c64-4cde-b775-775dfbeb35f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86e864e3-e40e-46b2-80b3-39d112720eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_run(question):\n",
    "    prompt=generate_prompt(question)\n",
    "    #prompt = get_simple_prompt(question)\n",
    "    max_new_tokens=200\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_new_tokens,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b50dc360-ed07-4ddc-9e92-bb57e848cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Give me a summary of the abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a131cfe3-d012-4218-b8bc-e9f4ce1bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c83b09f2-be9c-44b6-90f9-66ac468d04a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9eefb-5cac-4cfb-a9f7-91c25968fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2500fdd4-1d20-4196-9e60-19446c08f526",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "034bfaee-c221-4cb6-97b9-484a6cef8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f7f733c-93f3-4eb0-8e60-fec26ac246a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454f235426a84e6d990ae4f043273b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    model = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "667b5933-1608-4fe6-8f6c-32a38aa1e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_token_length(prompt: str) -> int:\n",
    "    #prompt = get_simple_prompt(question)\n",
    "    input_ids = tokenizer([prompt], return_tensors='np', add_special_tokens=False)['input_ids']\n",
    "    return input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eecaca69-857f-4921-8271-6bbc024c09ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_input_token_length(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0c7c001-c232-4dcc-8f79-f7019289fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(question: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.90,\n",
    "        top_k: int = 20,\n",
    "        repetition_penalty=1.15,\n",
    "       ) -> Iterator[str]:\n",
    "    prompt = get_simple_prompt(question)\n",
    "    max_new_tokens=get_input_token_length(prompt)\n",
    "    inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "    streamer = TextIteratorStreamer(tokenizer,\n",
    "                                    timeout=10.,\n",
    "                                    skip_prompt=True,\n",
    "                                    skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        temperature=temperature,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield ''.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "409a030b-e7f3-410f-96de-de76b37ec950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me a summary of the abstract'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45c46d24-9a9e-49a6-abb1-c4e6ebf0cb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object run at 0x000001CCD8A01540>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1b50791-3371-437d-a640-826681333dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    message: str,\n",
    "    chat_history: list[tuple[str, str]],\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.90,\n",
    "    top_k: int = 20,\n",
    "    repetition_penalty=1.15,    \n",
    ") -> Iterator[list[tuple[str, str]]]:\n",
    "    generator = run(message, max_new_tokens, temperature, top_p, top_k,repetition_penalty , )\n",
    "    try:\n",
    "        first_response = next(generator)\n",
    "        yield history + [(message, first_response)]\n",
    "    except StopIteration:\n",
    "        yield history + [(message, '')]\n",
    "    for response in generator:\n",
    "        yield history + [(message, response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7541b2c-9805-4955-b00f-2af28643f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(message: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    try:\n",
    "        generator = generate(message, [], 1024, 1, 0.95, 50,1.5)\n",
    "    except Exception as error:\n",
    "      print(\"An error occurred:\", error) # An error occurred: name 'x' is not defined        \n",
    "            \n",
    "    for x in generator:\n",
    "        pass\n",
    "    return '', x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "842d3d55-f53e-4e71-9d7f-6a00414a7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_example(\"Give me a summary of the abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d567cd-70e2-4853-bba2-fbfe04e60d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ArxivChat)",
   "language": "python",
   "name": "arxivchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
