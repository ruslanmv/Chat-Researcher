{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba2ab0e-31c2-4bc9-a66e-0596cca01f6e",
   "metadata": {},
   "source": [
    "# Arxiv Accelerator\n",
    "In this notebook we are going to develop a simple gradio application that will search papers and will analize it with Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca46daf7-7aa6-44c1-82f0-b7f4500911b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import fitz\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import gradio as gr\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Compability with Hugging Face models\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "import re\n",
    "# Compability with ChatGPT\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb647-0671-46b9-aff1-35a0326c2377",
   "metadata": {},
   "source": [
    "# Step 1 - PDF Analizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb65f71-d963-409a-878e-96d837bba211",
   "metadata": {},
   "source": [
    "The first thing that we want to create is the program that download and read and summarize papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1aede3-915d-4063-b535-d640ed073a6b",
   "metadata": {},
   "source": [
    "<img title=\"a title\" alt=\"Alt text\" src=\"./assets/paper.png\">\n",
    "The first example that we will use in this notebookwill we  : \n",
    "Attention Is All You Need\n",
    "\n",
    "[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c9852d-fa73-4e7b-a398-db59317cb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce2d5b52-06f8-4470-8116-955052c4550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, output_path):\n",
    "    urllib.request.urlretrieve(url, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a5b60cb-7818-4290-a393-7832450a8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_file = \"document.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adbb0588-e6da-4eae-b735-0a2ca2601509",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdf(url, downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9ff4321-f8b4-4127-9ac7-dcab388212ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path, start_page=1, end_page=None):\n",
    "    doc = fitz.open(path)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    if end_page is None:\n",
    "        end_page = total_pages\n",
    "\n",
    "    text_list = []\n",
    "\n",
    "    for i in range(start_page-1, end_page):\n",
    "        text = doc.load_page(i).get_text(\"text\")\n",
    "        text = preprocess(text)\n",
    "        text_list.append(text)\n",
    "\n",
    "    doc.close()\n",
    "    return text_list\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75382e2e-734b-4ac3-9dc6-24ddb2a6077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = pdf_to_text(downloaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c14fa2d-d9e8-49b9-a35f-61b998811e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "632e91d5-d5ab-474b-95ea-80b7bdd5dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_chunks(texts, word_length=150, start_page=1):\n",
    "    text_toks = [t.split(' ') for t in texts]\n",
    "    page_nums = []\n",
    "    chunks = []\n",
    "    \n",
    "    for idx, words in enumerate(text_toks):\n",
    "        for i in range(0, len(words), word_length):\n",
    "            chunk = words[i:i+word_length]\n",
    "            if (i+word_length) > len(words) and (len(chunk) < word_length) and (\n",
    "                len(text_toks) != (idx+1)):\n",
    "                text_toks[idx+1] = chunk + text_toks[idx+1]\n",
    "                continue\n",
    "            chunk = ' '.join(chunk).strip()\n",
    "            chunk = f'[Page no. {idx+start_page}]' + ' ' + '\"' + chunk + '\"'\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb182ffb-1c25-4e3f-9f13-66e65c141b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks= text_to_chunks(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7a390f5-ff16-4266-bbe6-02e660d206b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6d5e04-be12-4df5-be16-834e6a7982ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 41 pieces of the article now\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} pieces of the article now\".format(parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bba1f9-d49d-480d-aa8a-2abce2da5cb8",
   "metadata": {},
   "source": [
    "Now above all the list of 41 pieces, we should reduce the amount of pieces, this is possible by usuing the Semantic Search.\n",
    "Thie is great model used by Google that his the universal sentence encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8191383d-3a4a-4867-aebc-f10d8449c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self):\n",
    "        self.use = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "        self.fitted = False\n",
    "    def fit(self, data, batch=1000, n_neighbors=5):\n",
    "        self.data = data\n",
    "        self.embeddings = self.get_text_embedding(data, batch=batch)\n",
    "        n_neighbors = min(n_neighbors, len(self.embeddings))\n",
    "        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.nn.fit(self.embeddings)\n",
    "        self.fitted = True\n",
    "    def __call__(self, text, return_data=True):\n",
    "        inp_emb = self.use([text])\n",
    "        neighbors = self.nn.kneighbors(inp_emb, return_distance=False)[0]\n",
    "        if return_data:\n",
    "            return [self.data[i] for i in neighbors]\n",
    "        else:\n",
    "            return neighbors\n",
    "    def get_text_embedding(self, texts, batch=1000):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch):\n",
    "            text_batch = texts[i:(i+batch)]\n",
    "            emb_batch = self.use(text_batch)\n",
    "            embeddings.append(emb_batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664427fc-a4db-4c00-aaf6-c9ebdf58d8eb",
   "metadata": {},
   "source": [
    "We summarize all the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c8ed9b3-f0a0-42fe-85a6-d32acb5fa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = SemanticSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "852f8c3f-ae76-4e7e-b7fd-de5f0fae30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender(path, start_page=1):\n",
    "    global recommender\n",
    "    texts = pdf_to_text(path, start_page=start_page)\n",
    "    chunks = text_to_chunks(texts, start_page=start_page)\n",
    "    recommender.fit(chunks)\n",
    "    return 'Corpus Loaded.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47fe89d6-b8ca-43e4-8171-b576fe5e01ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nJIT compilation failed.\n\t [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_restored_function_body_4561]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mload_recommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownloaded_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m, in \u001b[0;36mload_recommender\u001b[1;34m(path, start_page)\u001b[0m\n\u001b[0;32m      3\u001b[0m texts \u001b[38;5;241m=\u001b[39m pdf_to_text(path, start_page\u001b[38;5;241m=\u001b[39mstart_page)\n\u001b[0;32m      4\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_to_chunks(texts, start_page\u001b[38;5;241m=\u001b[39mstart_page)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mrecommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorpus Loaded.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m, in \u001b[0;36mSemanticSearch.fit\u001b[1;34m(self, data, batch, n_neighbors)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_neighbors, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors)\n",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m, in \u001b[0;36mSemanticSearch.get_text_embedding\u001b[1;34m(self, texts, batch)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch):\n\u001b[0;32m     22\u001b[0m     text_batch \u001b[38;5;241m=\u001b[39m texts[i:(i\u001b[38;5;241m+\u001b[39mbatch)]\n\u001b[1;32m---> 23\u001b[0m     emb_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb_batch)\n\u001b[0;32m     25\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(embeddings)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:686\u001b[0m, in \u001b[0;36m_call_attribute\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_attribute\u001b[39m(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 686\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\nJIT compilation failed.\n\t [[{{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/mod}}]] [Op:__inference_restored_function_body_4561]"
     ]
    }
   ],
   "source": [
    "load_recommender(downloaded_file , start_page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7021477f-9449-4e39-9456-cebb2c639890",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Give me a summary of the abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d152574-8c8f-43b4-ab81-2a55276d3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question):\n",
    "    topn_chunks = recommender(question)\n",
    "    results = \"\"\n",
    "    results += 'search results:\\n\\n'\n",
    "    for c in topn_chunks:\n",
    "        results += c + '. \\n\\n'       \n",
    "    instruction = \"Instructions: Compose a comprehensive reply to the query using the search results given. \"\\\n",
    "              \"Cite each reference using [ Page Number] notation (every result has this number at the beginning). \"\\\n",
    "              \"Citation should be done at the end of each sentence. If the search results mention multiple subjects \"\\\n",
    "              \"with the same name, create separate answers for each. Only include information found in the results and \"\\\n",
    "              \"don't add any additional information. Make sure the answer is correct and don't output false content. \"\\\n",
    "              \"If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \"\\\n",
    "              \"search results which has nothing to do with the question. Only answer what is asked. The \"\\\n",
    "              \"answer should be short and concise.\"\n",
    "    \n",
    "    prompt = instruction + \"\\n\\nQuery: {}\".format(question) + \" \\n\\n\" + results + \" \\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d308d2-d2f9-4161-891a-43509a9205ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt3(question):\n",
    "    topn_chunks = recommender(question)\n",
    "    results = \"\"\n",
    "    results += 'search results:\\n\\n'\n",
    "    for c in topn_chunks:\n",
    "        results += c + '. \\n\\n'       \n",
    "    instruction = \"Instructions: Compose a comprehensive reply to the query using the search results given. \"\\\n",
    "              \"Cite each reference using [ Page Number] notation (every result has this number at the beginning). \"\\\n",
    "              \"Citation should be done at the end of each sentence. If the search results mention multiple subjects \"\\\n",
    "              \"with the same name, create separate answers for each. Only include information found in the results and \"\\\n",
    "              \"don't add any additional information. Make sure the answer is correct and don't output false content. \"\\\n",
    "              \"If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \"\\\n",
    "              \"search results which has nothing to do with the question. Only answer what is asked. The \"\\\n",
    "              \"answer should be short and concise.\"\\\n",
    "              \"Do not include the instructions in the answer.\"\n",
    "    \n",
    "    prompt = instruction + \"\\n\\nQuery: {}\".format(question) + \" \\n\\n\" + results + \" \\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707ed074-0b1f-405a-ab62-7a73a7bc875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=generate_prompt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3143d6e-f7e0-4d9d-976d-71e43716b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=generate_prompt3(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "290f6bae-36cf-40ae-9e0d-45a3e2b984ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\Compose a comprehensive reply to the query using the search results given. \n",
    "Cite each reference using [ Page Number] notation (every result has this number at the beginning). \n",
    "Citation should be done at the end of each sentence. If the search results mention multiple subjects \n",
    "with the same name, create separate answers for each. Only include information found in the results and \n",
    "don't add any additional information. Make sure the answer is correct and don't output false content. \n",
    "If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \n",
    "search results which has nothing to do with the question. Only answer what is asked. The \n",
    "answer should be short and concise.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc85d3fd-6f54-4264-9e0c-a03bdda5ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(message: str, chat_history: list[tuple[str, str]],\n",
    "               system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    # The first user input is _not_ stripped\n",
    "    do_strip = False\n",
    "    for user_input, response in chat_history:\n",
    "        user_input = user_input.strip() if do_strip else user_input\n",
    "        do_strip = True\n",
    "        texts.append(f'{user_input} [/INST] {response.strip()} </s><s>[INST] ')\n",
    "    message = message.strip() if do_strip else message\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c2270a0-337c-4295-92b8-d6d90eee5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_prompt(question: str) -> str:\n",
    "    topn_chunks = recommender(question)\n",
    "    results = \"\"\n",
    "    results += 'Search results:\\n\\n'\n",
    "    for c in topn_chunks:\n",
    "        results += c + '. \\n\\n' \n",
    "    message =  \"\\n\\nQuery: {}\".format(question) + \" \\n\\n\" + results + \" \\nAnswer:\"\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{DEFAULT_SYSTEM_PROMPT}\\n<</SYS>>\\n\\n']\n",
    "    texts.append(f'{message} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "582d46f5-8a74-4373-b8b6-620149607edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2=get_simple_prompt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f76224-f139-4c7f-bef4-52c49c4f4505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\n\\\\Compose a comprehensive reply to the query using the search results given. \\nCite each reference using [ Page Number] notation (every result has this number at the beginning). \\nCitation should be done at the end of each sentence. If the search results mention multiple subjects \\nwith the same name, create separate answers for each. Only include information found in the results and \\ndon\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. \\nIf the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier \\nsearch results which has nothing to do with the question. Only answer what is asked. The \\nanswer should be short and concise.\\n<</SYS>>\\n\\n\\n\\nQuery: Give me a summary of the abstract \\n\\nSearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2  Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the\". \\n\\n[Page no. 10] \"structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9  Table 4: The Transformer generalizes well to English constituency parsing (Results are\". \\n\\n[Page no. 2] \"recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all\". \\n\\n[Page no. 7] \"any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6  length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output\". \\n\\n \\nAnswer: [/INST]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f71b0a-a0a4-47d1-a325-65e6d9740528",
   "metadata": {},
   "source": [
    "# Modeling with LLama 2 \n",
    "Python code that answers questions from a PDF file using the transformers meta-llama/Llama-2-7b-chat-hf model and Retrieval Augmenten (RAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d920b50d-10f6-42a3-85d3-90ed772a5ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af4befa1c394e8ab629bb6e25166976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 241 at dim 1 (got 389)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the abstract?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Generate text using the model\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m abstract_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Print the generated text\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(abstract_text)\n",
      "Cell \u001b[1;32mIn[61], line 31\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(tokenized_text, model, prompt)\u001b[0m\n\u001b[0;32m     28\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert the input IDs to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Generate text using the model\u001b[39;00m\n\u001b[0;32m     34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     35\u001b[0m   input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     36\u001b[0m   attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m   top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 241 at dim 1 (got 389)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocess the text by removing stop words and non-alphabetic characters.\"\"\"\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "  tokens = [t for t in tokens if t.isalpha()]\n",
    "  tokens = [t for t in tokens if t.islower()]\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# Generate text using the model\n",
    "def generate_text(tokenized_text, model, prompt):\n",
    "  \"\"\"Generate text using the given model and prompt.\"\"\"\n",
    "\n",
    "  # Tokenize the prompt\n",
    "  input_ids = tokenized_text['input_ids']\n",
    "  attention_mask = tokenized_text['attention_mask']\n",
    "  prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "\n",
    "  # Convert the input IDs to a PyTorch tensor\n",
    "  input_ids = torch.tensor(input_ids)\n",
    "\n",
    "  # Generate text using the model\n",
    "  outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=512,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=2.0,\n",
    "    top_p=0.9\n",
    "  )\n",
    "\n",
    "  # Decode the generated tokens into text\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  return generated_text\n",
    "\n",
    "# Load the PDF and preprocess the text\n",
    "reader = PdfReader('document.pdf')\n",
    "text_list = []\n",
    "for page_number in range(len(reader.pages)):\n",
    "  page = reader.pages[page_number]\n",
    "  text = page.extractText()\n",
    "  text = preprocess_text(text)\n",
    "  text += '[Page no. {}]'.format(page_number+1)\n",
    "  text_list.append(text)\n",
    "\n",
    "# Model name\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_text = tokenizer(text_list, truncation=False, padding=False)\n",
    "\n",
    "# Create a model for causal language modeling\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = 'What is the abstract?'\n",
    "\n",
    "# Generate text using the model\n",
    "abstract_text = generate_text(tokenized_text, model, prompt)\n",
    "\n",
    "# Print the generated text\n",
    "print(abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9cb3557-b1ff-41a0-bb9f-a31757eac42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2124bad7ffbb49d29a566a20fda4b623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizerFast' object has no attribute 'truncate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the abstract?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Generate text using the model\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m abstract_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Print the generated text\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(abstract_text)\n",
      "Cell \u001b[1;32mIn[63], line 31\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(tokenized_text, model, prompt)\u001b[0m\n\u001b[0;32m     28\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Truncate the input sequence to 241 tokens\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m241\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert the input IDs to a PyTorch tensor\u001b[39;00m\n\u001b[0;32m     34\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_ids)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaTokenizerFast' object has no attribute 'truncate'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocess the text by removing stop words and non-alphabetic characters.\"\"\"\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "  tokens = [t for t in tokens if t.isalpha()]\n",
    "  tokens = [t for t in tokens if t.islower()]\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# Generate text using the model\n",
    "def generate_text(tokenized_text, model, prompt):\n",
    "  \"\"\"Generate text using the given model and prompt.\"\"\"\n",
    "\n",
    "  # Tokenize the prompt\n",
    "  input_ids = tokenized_text['input_ids']\n",
    "  attention_mask = tokenized_text['attention_mask']\n",
    "  prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "\n",
    "  # Truncate the input sequence to 241 tokens\n",
    "  input_ids = tokenizer.truncate(input_ids, max_length=241)\n",
    "\n",
    "  # Convert the input IDs to a PyTorch tensor\n",
    "  input_ids = torch.tensor(input_ids)\n",
    "\n",
    "  # Generate text using the model\n",
    "  outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=512,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=2.0,\n",
    "    top_p=0.9\n",
    "  )\n",
    "\n",
    "  # Decode the generated tokens into text\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  return generated_text\n",
    "\n",
    "# Load the PDF and preprocess the text\n",
    "reader = PdfReader('document.pdf')\n",
    "text_list = []\n",
    "for page_number in range(len(reader.pages)):\n",
    "  page = reader.pages[page_number]\n",
    "  text = page.extractText()\n",
    "  text = preprocess_text(text)\n",
    "  text += '[Page no. {}]'.format(page_number+1)\n",
    "  text_list.append(text)\n",
    "\n",
    "# Model name\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_text = tokenizer(text_list, truncation=False, padding=False)\n",
    "\n",
    "# Create a model for causal language modeling\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = 'What is the abstract?'\n",
    "\n",
    "# Generate text using the model\n",
    "abstract_text = generate_text(tokenized_text, model, prompt)\n",
    "\n",
    "# Print the generated text\n",
    "print(abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e05fd-786a-42a5-bd31-1df975dda1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05f08ed6-8aec-40f3-a404-9fb66e2cd611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08a7eb36b0b41bdba351332b3e12f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object has no attribute 'shape'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocess the text by removing stop words and non-alphabetic characters.\"\"\"\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "  tokens = [t for t in tokens if t.isalpha()]\n",
    "  tokens = [t for t in tokens if t.islower()]\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# Generate text using the model\n",
    "def generate_text(tokenized_text, model, prompt):\n",
    "  \"\"\"Generate text using the given model and prompt.\"\"\"\n",
    "  # Tokenize the prompt\n",
    "  input_ids = tokenized_text['input_ids']\n",
    "  attention_mask = tokenized_text['attention_mask']\n",
    "  prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "\n",
    "  # Generate text using the model\n",
    "  try:\n",
    "    outputs = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      max_length=512,\n",
    "      temperature=1.0,\n",
    "      repetition_penalty=2.0,\n",
    "      top_p=0.9\n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return ''\n",
    "\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  return generated_text\n",
    "\n",
    "# Load the PDF and preprocess the text\n",
    "reader = PdfReader('document.pdf')\n",
    "text_list = []\n",
    "for page_number in range(len(reader.pages)):\n",
    "  page = reader.pages[page_number]\n",
    "  text = page.extractText()\n",
    "  text = preprocess_text(text)\n",
    "  text += '[Page no. {}]'.format(page_number+1)\n",
    "  text_list.append(text)\n",
    "\n",
    "# Model name\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_text = tokenizer(text_list, truncation=False, padding=False)\n",
    "\n",
    "# Create a model for causal language modeling\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = 'What is the abstract?'\n",
    "\n",
    "# Generate text using the model\n",
    "abstract_text = generate_text(tokenized_text, model, prompt)\n",
    "\n",
    "# Print the generated text\n",
    "print(abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb246c32-f7ed-46ed-9dc0-0c0d788e1040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4030ff9f-e0bf-45e5-b98d-db38486833ed",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "https://stackoverflow.com/questions/76772509/llama-2-7b-hf-repeats-context-of-question-directly-from-input-prompt-cuts-off-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a87aeb6-d07d-405c-870e-4fd79dba05c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771f15bd6d634dbd9fd5b1c94748c9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f03738a6-faad-455c-849f-5fbf7365c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_question_1(question):\n",
    "    sequences = pipeline(\n",
    "        '{}\\n'.format(question),\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a153af9-7719-432a-be77-f91b348db164",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_impl_cpu_\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmake_question_1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of Italy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m, in \u001b[0;36mmake_question_1\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_question_1\u001b[39m(question):\n\u001b[1;32m----> 2\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:205\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:268\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    269\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1641\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1642\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1643\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1644\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1645\u001b[0m     )\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1649\u001b[0m         input_ids,\n\u001b[0;32m   1650\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1651\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1652\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1653\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1654\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1655\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1656\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1657\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1658\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1659\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1660\u001b[0m     )\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1663\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1664\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1665\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1666\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1671\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1672\u001b[0m     )\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2727\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2730\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2731\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2732\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2733\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2734\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2735\u001b[0m )\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2738\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    817\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    819\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    701\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    703\u001b[0m         hidden_states,\n\u001b[0;32m    704\u001b[0m         attention_mask,\n\u001b[0;32m    705\u001b[0m         position_ids,\n\u001b[0;32m    706\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:424\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:321\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    318\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(value_states, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m    323\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mD:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "make_question_1(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "690f0506-b131-4f2b-9c1a-c1b23b58ca92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51950fccc74e444b9884a9836441aefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Define the model name (Llama-2 7B-hf)\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\" #model requires at least 12GB of GPU memory to run. \n",
    "#model_name ='EleutherAI/gpt-neo-2.7B' #16GB RAM NEEDED\n",
    "#model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b14be267-ebb7-4d0d-b473-e0043656052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0554dc94-6456-4644-ac9d-b4722e5e27ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is the capital of Italy?\\nWhat is Italy's currency?\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f615ea1-ce8c-45df-8188-7a8193a4762c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lama2 7b\n",
    "'''\n",
    "['What is the capital of Italy?\\n\\nThe capital city of the Italian Republic is Rome (Roma in Italian). Rome is located in central-western Italy and is home to many famous landmarks such as the Colosseum, the Pantheon, and the Vatican City. Rome has a population of over 2.8 million people and has been the seat of Italian government since 1865.']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9184d5bb-6ec3-4a72-a489-28f91ce351e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "- Rome\n",
      "The capital city of the Italian Republic is Rome. Rome is a special city that has a unique administrative status within Italy. It is home to the national government, the Prime Minister's office, and many other national institutions, but it is also a regional entity in its own right. As such, it has its mayor and local administration, as well as a number of special powers and responsibilities that set it apart from other Italian cities\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c42daf-bae0-4e98-b8f4-427a1a09342e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a17bf-a58d-4d24-a8ac-58180d65367b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1644e3ba-107b-491a-897e-aacdffd430d9",
   "metadata": {},
   "source": [
    "If we want to avoid repetition of the input , we can encode the prompt using Llama tokenizer, find the length of the prompt token ids and remove them from the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae904d49-7a73-4d5b-a259-3ca515820cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text_clean(prompt):\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    start_index = prompt_tokens.shape[-1]\n",
    "    output = model.generate(prompt_tokens, num_return_sequences=1)\n",
    "    generation_output = output[0][start_index:]\n",
    "    generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    return generation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd4de50-ecb7-43ae-b75a-1d8fce60f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe capital of Italy is Rome, the largest city in'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc1e748-5c94-4b7d-88ef-f86e74a6a384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe capital of Italy is Rome (Roma).'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551ff15-6a8b-43aa-aa2b-37d53fbdcdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e60c8b3-a810-4559-b97a-77ff9b8694de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-att'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfa88cb7-2245-4232-9272-d67155033cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ention mechanism, which is a key component of the Transformer architecture, allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is in contrast to traditional recurrent neural network (RNN) architectures, which only consider the previous elements in the sequence when making predictions. The Transformer architecture has been shown to be highly effective in a variety of natural language processing tasks, including machine translation and text generation\".\\n\\n[Page no. 4] \"The Transformer architecture was introduced in a paper by Vaswani et al. in 2017. The authors proposed a new neural network architecture that uses self-attention mechanisms to process input sequences in parallel, rather than sequentially as in traditional RNNs. The Transformer architecture has since become widely used in natural language processing tasks, including machine translation, text generation, and question answering\".\\n\\n[Page no. 5] \"The Transformer architecture is based on the idea of self-attention, which allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is in contrast to traditional RNNs, which only consider the previous elements in the sequence when making predictions. The Transformer architecture has been shown to be highly effective in a variety of natural language processing tasks, including machine translation and text generation\".\\n\\nSummary: The Transformer architecture is a new neural network architecture that uses self-attention mechanisms to process input sequences in parallel, rather than sequentially as in traditional RNNs. It has been shown to be highly effective in a variety of natural language processing tasks, including machine translation and text generation. The Transformer architecture dispenses with recurrence and convolutions entirely, relying solely on attention mechanisms to process input sequences.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(prompt3[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e7c463b-8f84-48f1-a4ce-51cb19e5034d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 456, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ention'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean(prompt3[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4d300da-b0b7-47da-ba1e-4c707b837371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text_clean_new(prompt):\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    start_index = prompt_tokens.shape[-1]\n",
    "    output = model.generate(prompt_tokens, \n",
    "                            num_return_sequences=1,\n",
    "                            max_length=457)\n",
    "    generation_output = output[0][start_index:]\n",
    "    generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    return generation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6f0a341-505c-47bb-a1fd-bbf8a6c4d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ention'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_clean_new(prompt3[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61afcf2-e12f-4a4e-a454-916f03018de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68e8ae5214f453488f3849c952d81e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_new_tokens=500):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    start_index = input_ids.shape[-1]\n",
    "    # Generate text with attention_mask and increased max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, num_return_sequences=1, max_new_tokens=max_new_tokens)\n",
    "    generation_output = output[0][start_index:]\n",
    "    generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "    return generation_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c07d710-d727-4861-8787-1cbe70a2a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe capital of Italy is Rome (Roma).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"What is the capital of Italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b1d5bf-134c-4ed9-be72-f5ca8b713219",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruc='Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-att'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6404ee-1822-4099-a3ce-bc225cab5a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ention mechanism, which is a key component of the Transformer efficiency. Unlike traditional recurrent neural networks (RNNs), the Transformer does not rely on recurrence to process sequences. Instead, it uses a self-attention mechanism to parallelize the computation of attention across all positions in a sequence. This allows the Transformer to process long sequences efficiently and scale to larger models\".\\n\\n[Page no. 5] \"In this paper, we introduce the Transformer, a new architecture for sequence-to-sequence translation that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with attention mechanisms. The Transformer relies solely on self-attention mechanisms to process sequences, allowing it to parallelize the computation of attention across all positions in a sequence. This allows the Transformer to efficiently process long sequences and scale to larger models\".\\n\\n\\nAnswer: Based on the search results provided, the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(instruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5e71f9-09ef-4ebf-bdd3-a663c9e5599f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ention is a mechanism that allows the model to focus on specific parts of the input sequence when generating the output. This is particularly useful in natural language processing tasks, where the input sequences can be long and unstructured. In this paper, we propose a new attention mechanism called multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This allows the model to capture a wider range of contextual relationships between different parts of the input sequence, leading to improved performance on a variety of natural language processing tasks\".\\n\\n[Page no. 5] \"attention is a key component of many state-of-the-art natural language processing models, including the Transformer architecture proposed in this paper. Attention allows the model to selectively focus on certain parts of the input sequence when generating the output, rather than using a fixed context or relying on the entire input sequence. This can be particularly useful in tasks such as machine translation, where the input sequence can be long and unstructured. In this paper, we propose a new attention mechanism called multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This allows the model to capture a wider range of contextual relationships between different parts of the input sequence, leading to improved performance on a variety of natural language processing tasks\".\\n\\n\\nFound Nothing.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(instruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c4cea-880a-4246-b7d8-e87915ccf1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0c5fe7-f24e-4d31-9e16-383e8f22794c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3bb0f895bd45ebbe556fa850d0fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rusla\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb9646dff0e4d9b89038cd5a89eb5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59431974e88c4c47b3a3606c59f6a6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c494601b5964ee5a0d0571df6f76207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee5564a52b4fd3a353a2be6d1ba084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "\n",
      "The capital is Rome.\n",
      "...\n",
      "\"The city of Rome is a city in which the people are united in a common interest. It is not a place where the rich and the poor live side by side. The people of the city are not divided by class, but by race, language, and religion. Rome has a great tradition of democracy, which is reflected in the fact that the Roman people have always been the most democratic of all\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Load the pre-trained model and tokenizer\n",
    "#The following GPT-2 models from Hugging Face are compatible with GPT2LMHeadModel:\n",
    "#model_name = \"gpt2\"\n",
    "#model_name = \"gpt2-medium\"\n",
    "model_name = \"gpt2-large\"\n",
    "#model_name = \"gpt2-xl\"\n",
    "#model_name = \"gpt2-distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    return generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d8fdd-fa25-4df4-812b-2728d94af2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141158dd-1bea-4e21-a596-0d3a2e2d0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "D:\\ArxivChat\\.arxiv_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "\n",
      "The capital is Rome.\n",
      "...\n",
      "\"The city of Rome is a city in which the people are united in a common interest. It is not a place where the rich and the poor live side by side. The people of the city are not divided by class, but by race, language, and religion. Rome has a great tradition of democracy, which is reflected in the fact that the Roman people have always been the most democratic of all\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is a chatbot?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is a chatbot?\n",
      "\n",
      "A chat bot is an artificial intelligence program that can be programmed to respond to a user's questions and provide answers.\n",
      ". A chat robot is not a person. It is programmed with the intention of answering questions. The user can ask the bot questions, and the chatbots will respond. This is similar to the way a human would ask a question. However, the user is the one who is interacting with a bot. In this case, a conversation is\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Generate text with attention_mask\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    return generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e9f460-754a-4811-8b35-97896d0e5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  what is the capital of Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. ? Rome.\n",
      "\n",
      "The capital is Rome, and the city is a city. It is not a country. Rome is an island. The city of Rome has no country, no state, nor a government. There is no government, but there is government in Rome and it is called the Roman Republic. This is why the Romans called it the Republic, because it was a republic. But the word \"Republic\" is used in the sense of a state. So the term \"Roman Republic\" means a political entity, not an entity. And the name \"Rome\" does not mean a place. \"The city\" in this case means the whole of the country of which Rome was the center. In other words, the \"city\" of \"the city,\" the entire country is referred to as \" Rome.\"\n",
      "...\n",
      "In the same way, in a sense, \"America\" refers to the United States of America. That is, it refers not to a particular place\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=200, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Generate text with attention_mask and max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=max_length + 200,  # Increase max_length to accommodate the generated text\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=200,  # Set max_new_tokens to control total tokens generated\n",
    "        )\n",
    "\n",
    "    # Decode and print generated text without the prompt\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True)[len(prompt):] for output_seq in output]\n",
    "    return generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aca1fef-1acd-4a7b-a42c-1044e8a91602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  what is the capital of Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=406) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. what is the capital of Italy, Rome?' \"\n",
      "\n",
      "The question that was raised was: \"How many people live in Rome?\"\n",
      ", or \"Who has the biggest city in Italy?\".\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=407) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy? Rome. Rome is one of the most beautiful cities in the world. It's a great city. I've lived in Italy for the last 20 years. That's why I'm here.\"\n",
      "\n",
      "His next stop: the Vatican.\n",
      ", a former president of Germany, is scheduled to attend the first meeting of a Vatican advisory board on Saturday, and his trip to Rome could be the subject of an international media frenzy. As a result, the Italian press has been busy speculating about what a pope's visit to the Holy See will mean for Europe and the Catholic Church. What is he going to do there? What will he talk about? And what is his message for world Catholics?\n",
      "...\n",
      "The pope will meet with heads of state and government from the United States, Russia and Japan, as well as religious leaders from several African nations. He will also meet Pope Benedict XVI, who is retiring after a papacy that lasted more than five decades. \"He is a man of great experience,\" said an Italian diplomat who attended the meeting. But he'll also encounter the heads and shoulders of world Catholicism, including the pope himself. The Vatican has put together an advisory council of about 50 experts from around the globe, which includes Catholics from Africa, Asia, Europe, Latin America and North America. Some are former Vatican officials, others are not. They will convene in Rome on April 23.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=400, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Calculate the number of tokens already present in the input prompt\n",
    "    prompt_token_count = input_ids.shape[1]\n",
    "\n",
    "    # Calculate the total number of tokens to be generated\n",
    "    total_max_tokens = max_length + prompt_token_count\n",
    "\n",
    "    # Generate text with attention_mask and max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=total_max_tokens,  # Set max_length to include both prompt and generated text\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=400,  # Set max_new_tokens to control total tokens generated\n",
    "            do_sample=True,  # This flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "        )\n",
    "\n",
    "    # Define a delimiter to separate the instruction from the generated content\n",
    "    delimiter = \">>>\"\n",
    "    \n",
    "    # Decode and print generated text without the prompt and instructions\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    cleaned_generated_text = []\n",
    "\n",
    "    for text in generated_text:\n",
    "        parts = text.split(delimiter)\n",
    "        if len(parts) >= 2:\n",
    "            # Extract the generated content and remove leading/trailing whitespaces\n",
    "            generated_content = parts[1].strip()\n",
    "            cleaned_generated_text.append(generated_content)\n",
    "        else:\n",
    "            # No delimiter found, use the entire generated text\n",
    "            cleaned_generated_text.append(text)\n",
    "\n",
    "    return cleaned_generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b6c61-1300-4c31-8189-af9f6fe4e218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1815b3ed-b999-4d8b-a915-bf1eab857db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruc='Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-att'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c091b5fc-fee9-4ade-9c97-2c7b76c193b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\n\\\\Compose a comprehensive reply to the query using the search results given. \\nCite each reference using [ Page Number] notation (every result has this number at the beginning). \\nCitation should be done at the end of each sentence. If the search results mention multiple subjects \\nwith the same name, create separate answers for each. Only include information found in the results and \\ndon\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. \\nIf the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier \\nsearch results which has nothing to do with the question. Only answer what is asked. The \\nanswer should be short and concise.\\n<</SYS>>\\n\\n\\n\\nQuery: Give me a summary of the abstract \\n\\nSearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly\". \\n\\n[Page no. 3] \"self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2  Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the\". \\n\\n[Page no. 10] \"structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9  Table 4: The Transformer generalizes well to English constituency parsing (Results are\". \\n\\n[Page no. 2] \"recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all\". \\n\\n[Page no. 7] \"any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6  length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output\". \\n\\n \\nAnswer: [/INST]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f77f3f75-6d98-4f62-a1b9-56a4375ae409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Instructions: Compose a comprehensive reply to the query using the search results given. Cite each reference using [ Page Number] notation (every result has this number at the beginning). Citation should be done at the end of each sentence. If the search results mention multiple subjects with the same name, create separate answers for each. Only include information found in the results and don\\'t add any additional information. Make sure the answer is correct and don\\'t output false content. If the text does not relate to the query, simply state \\'Found Nothing\\'. Ignore outlier search results which has nothing to do with the question. Only answer what is asked. The answer should be short and concise.Do not include the instructions in the answer.\\n\\nQuery: Give me a summary of the abstract \\n\\nsearch results:\\n\\n[Page no. 1] \"Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the DNN, that is able to learn to recognize the features of an image and is therefore able not only to achieve good performance on image recognition tasks but also to encode structured information such as images and text. Dense networks and recurrent networks are both used in machine learning. However, despite the fact that they are able, in some cases, to perform better than traditional models, they have several drawbacks, namely that the structure of a network is hard to understand, and that a large number of parameters are needed to tune the network. In this work, we propose an architecture that does away with all these disadvantages and provides a simple, yet powerful, neural network that can perform well in image classification tasks. This architecture is based mainly on the representation of features in a sparse space, which allows the recurrent network to be trained without any prior knowledge of images. Our architecture can be used to train a convnet with a learning rate of 0.5, a single-layer perceptron with an output of size 2,000, or a multi-layered perceptrons with weights of 2.0. With this architecture the model is capable of learning to correctly recognize images from a set of training examples. Furthermore, it can learn the feature representation and generate a human-readable output. Finally, our model can also be extended to include a classifier. While the performance of our system remains limited, its ability to represent and extract structured data from images can provide a substantial improvement to machine-learning algorithms. To our knowledge, this is the first time that such a model has been proposed for machine vision. Keywords: Density Maximization, Convolution, Embedding, Neural Networks, Recognition, Sparse Representation, VGG-16, Image Classification\\nPosted in: Computational Vision, Deep Learning, Vision Research\\nThe search engine Google is not the only one using image search. Many other search engines also use image searching. What is different about Google image searches? First of all,']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(instruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf29dc05-5123-42da-b0f6-9c73676d3f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of Italy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=407) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy?\n",
      "\n",
      "The capital, Rome, is in Italy, not in the Republic of Rome. In the Roman Republic, the capitol was in Rome and the senators and plebeians elected their local representatives in their own cities. The capital was the seat of government.\n",
      ", it is called the Italian city and not the 'capital' of the country. Rome is not a state in itself. It is a city in a region of country, and there is no city of its own. Italy has been a province of a larger country since ancient times.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  >>>What is the capital of Italy>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=408) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. What is the capital of Italy\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  What is the capital of italy?>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=400) and `max_length`(=409) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "1. Capital of Italia\n",
      "- In italia, we have capital city, the city of Rome.\n",
      "\n",
      "\n",
      "\n",
      "What about the rest of the world?\n",
      ", italian capital\n",
      ".\n",
      "\n",
      ".\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "Fix the following code :\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to '[PAD]'\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode for faster inference\n",
    "model.eval()\n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=400, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Calculate the number of tokens already present in the input prompt\n",
    "    prompt_token_count = input_ids.shape[1]\n",
    "\n",
    "    # Calculate the total number of tokens to be generated\n",
    "    total_max_tokens = max_length + prompt_token_count\n",
    "\n",
    "    # Generate text with attention_mask and max_new_tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention_mask\n",
    "            max_length=total_max_tokens,  # Set max_length to include both prompt and generated text\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=400,  # Set max_new_tokens to control total tokens generated\n",
    "            do_sample=True,  # This flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "        )\n",
    "\n",
    "    # Define a delimiter to separate the instruction from the generated content\n",
    "    delimiter = \">>>\"\n",
    "    \n",
    "    # Decode and print generated text without the prompt and instructions\n",
    "    generated_text = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    cleaned_generated_text = []\n",
    "\n",
    "    for text in generated_text:\n",
    "        parts = text.split(delimiter)\n",
    "        if len(parts) >= 2:\n",
    "            # Extract the generated content and remove leading/trailing whitespaces\n",
    "            generated_content = parts[1].strip()\n",
    "            cleaned_generated_text.append(generated_content)\n",
    "        else:\n",
    "            # No delimiter found, use the entire generated text\n",
    "            cleaned_generated_text.append(text)\n",
    "\n",
    "    return cleaned_generated_text\n",
    "\n",
    "# Prompt for user input and generate text\n",
    "while True:\n",
    "    user_prompt = input(\"Enter a prompt (type 'exit' to quit): \")\n",
    "    if user_prompt.lower() == 'exit':\n",
    "        break\n",
    "    generated_text = generate_text(user_prompt)\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    for i, text in enumerate(generated_text):\n",
    "        print(f\"{i + 1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32909a9e-9c64-4cde-b775-775dfbeb35f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86e864e3-e40e-46b2-80b3-39d112720eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_run(question):\n",
    "    prompt=generate_prompt(question)\n",
    "    #prompt = get_simple_prompt(question)\n",
    "    max_new_tokens=200\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_new_tokens,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b50dc360-ed07-4ddc-9e92-bb57e848cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Give me a summary of the abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a131cfe3-d012-4218-b8bc-e9f4ce1bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c83b09f2-be9c-44b6-90f9-66ac468d04a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9eefb-5cac-4cfb-a9f7-91c25968fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2500fdd4-1d20-4196-9e60-19446c08f526",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "034bfaee-c221-4cb6-97b9-484a6cef8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f7f733c-93f3-4eb0-8e60-fec26ac246a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454f235426a84e6d990ae4f043273b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "else:\n",
    "    model = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "667b5933-1608-4fe6-8f6c-32a38aa1e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_token_length(prompt: str) -> int:\n",
    "    #prompt = get_simple_prompt(question)\n",
    "    input_ids = tokenizer([prompt], return_tensors='np', add_special_tokens=False)['input_ids']\n",
    "    return input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eecaca69-857f-4921-8271-6bbc024c09ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_input_token_length(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0c7c001-c232-4dcc-8f79-f7019289fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(question: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.90,\n",
    "        top_k: int = 20,\n",
    "        repetition_penalty=1.15,\n",
    "       ) -> Iterator[str]:\n",
    "    prompt = get_simple_prompt(question)\n",
    "    max_new_tokens=get_input_token_length(prompt)\n",
    "    inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "    streamer = TextIteratorStreamer(tokenizer,\n",
    "                                    timeout=10.,\n",
    "                                    skip_prompt=True,\n",
    "                                    skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        temperature=temperature,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield ''.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "409a030b-e7f3-410f-96de-de76b37ec950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me a summary of the abstract'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45c46d24-9a9e-49a6-abb1-c4e6ebf0cb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object run at 0x000001CCD8A01540>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1b50791-3371-437d-a640-826681333dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    message: str,\n",
    "    chat_history: list[tuple[str, str]],\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.90,\n",
    "    top_k: int = 20,\n",
    "    repetition_penalty=1.15,    \n",
    ") -> Iterator[list[tuple[str, str]]]:\n",
    "    generator = run(message, max_new_tokens, temperature, top_p, top_k,repetition_penalty , )\n",
    "    try:\n",
    "        first_response = next(generator)\n",
    "        yield history + [(message, first_response)]\n",
    "    except StopIteration:\n",
    "        yield history + [(message, '')]\n",
    "    for response in generator:\n",
    "        yield history + [(message, response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7541b2c-9805-4955-b00f-2af28643f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(message: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    try:\n",
    "        generator = generate(message, [], 1024, 1, 0.95, 50,1.5)\n",
    "    except Exception as error:\n",
    "      print(\"An error occurred:\", error) # An error occurred: name 'x' is not defined        \n",
    "            \n",
    "    for x in generator:\n",
    "        pass\n",
    "    return '', x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "842d3d55-f53e-4e71-9d7f-6a00414a7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_example(\"Give me a summary of the abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d567cd-70e2-4853-bba2-fbfe04e60d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ArxivChat)",
   "language": "python",
   "name": "arxivchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
